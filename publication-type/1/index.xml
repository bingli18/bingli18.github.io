<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>1 | AutoAI Lab</title>
    <link>/publication-type/1/</link>
      <atom:link href="/publication-type/1/index.xml" rel="self" type="application/rss+xml" />
    <description>1</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 25 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/logo_hufbce6705a566d43a9610d78203895d17_430824_300x300_fit_lanczos_2.png</url>
      <title>1</title>
      <link>/publication-type/1/</link>
    </image>
    
    <item>
      <title>SPD: Semi-Supervised Learning and Progressive Distillation for 3-D Detection</title>
      <link>/publication/22-focustr-iros/</link>
      <pubDate>Tue, 25 Oct 2022 00:00:00 +0000</pubDate>
      <guid>/publication/22-focustr-iros/</guid>
      <description>&lt;p&gt;Abstract:
The feature pyramid, which is a vital component of the convolutional neural networks, plays a significant role in several perception tasks, including object detection for autonomous driving. However, how to better fuse multi-level and multi-sensor feature pyramids is still a significant challenge, especially for object detection. This paper presents a FocusTR (Focusing on the valuable features by multiple Transformers), which is a simple yet effective architecture, to fuse feature pyramid for the single-stream 2D detector and two-stream 3D detector. Specifically, FocusTR encompasses several novel self-attention mechanisms, including the spatial-wise boxAlign attention (SB) for low-level spatial locations, context-wise affinity attention (CA) for high-level context information, and level-wise attention for the multi-level feature. To alleviate self-attention’s computational complexity and slow training convergence, FocusTR introduces a low and high-level fusion (LHF) to reduce the computational parameters, and the Pre-LN to accelerate the training convergence.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Class-Level Confidence Based 3D Semi-Supervised Learning</title>
      <link>/publication/23-3d-semi-learning-wacv/</link>
      <pubDate>Tue, 18 Oct 2022 00:00:00 +0000</pubDate>
      <guid>/publication/23-3d-semi-learning-wacv/</guid>
      <description>&lt;p&gt;Abstract:
Current pseudo-labeling strategies in 3D semi-supervised learning (SSL) fail to adaptively incorporate each class’s learning difficulty and learning status variance. In this work, we practically demonstrate that 3D unlabeled data class-level confidence can represent the learning status. Based on this finding, we present a novel class-level confidence based 3D SSL method. We firstly propose a dynamic thresholding method based on each class learning status obtained from class-level confidence. Then, a re-sampling strategy is designed to re-balance the learning status based on that the better learning status a class/instance has, the less sample probability it has. To show the generality of our method in 3D SSL task, we conduct extensive experiments on 3D SSL classification and detection tasks. Our method significantly outperforms state-of-the-art counterparts for both 3D SSL classification and detection tasks in all datasets.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Disentangling Object Motion and Occlusion for Unsupervised Multi-frame Monocular Depth</title>
      <link>/publication/22-dynamicdepth/</link>
      <pubDate>Sun, 20 Mar 2022 00:00:00 +0000</pubDate>
      <guid>/publication/22-dynamicdepth/</guid>
      <description>&lt;p&gt;Abstract:
Conventional self-supervised monocular depth prediction methods are based on a static environment assumption, which leads to accuracy degradation in dynamic scenes due to the mismatch and occlusion problems introduced by object motions. Existing dynamic-object-focused methods only partially solved the mismatch problem at the training loss level. In this paper, we accordingly propose a novel multi-frame monocular depth prediction method to solve these problems at both the prediction and supervision loss levels. Our method, called DynamicDepth, is a new framework trained via a self-supervised cycle consistent learning scheme. A Dynamic Object Motion Disentanglement (DOMD) module is proposed to disentangle object motions to solve the mismatch problem. Moreover, novel occlusion-aware Cost Volume and Re-projection Loss are designed to alleviate the occlusion effects of object motions. Extensive analyses and experiments on the Cityscapes and KITTI datasets show that our method significantly outperforms the state-of-the-art monocular depth prediction methods, especially in the areas of dynamic objects. Our code will be made publicly available.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SeeWay: Vision-Language Assistive Navigation for the Visually Impaired</title>
      <link>/publication/22-seeway-smc/</link>
      <pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate>
      <guid>/publication/22-seeway-smc/</guid>
      <description>&lt;p&gt;Abstract:
Assistive navigation for blind or visually impaired (BVI) individuals is of significance to extend their mobility and safety in traveling, enhancing their employment opportunities and fostering personal fulfillment. Conventional research is mainly based on robotic navigation approaches through localization, mapping, and path planning frameworks. They require heavy manual annotation of semantic information in maps and its alignment with sensor mapping. Inspired by the fact that we human beings naturally rely on language instruction inquiry and visual scene understanding to navigate in an unfamiliar environment, this paper proposes a novel vision-language model-based approach for BVI navigation. It does not need heavy-labeled indoor maps and provides a Safe and Efficient E-Wayfinding (SeeWay) assistive solution for BVI individuals. The system consists of a scene-graph map construction module, a navigation path generation module for global path inference by vision-language navigation (VLN), and a navigation with obstacle avoidance module for real-time local navigation. The SeeWay system was deployed on portable iPhone devices with cloud computing assistance for the VLN model inference. The field tests show the effectiveness of the VLN global path finding and local path re-planning. Experiments and quantitative results reveal that heuristic-style instruction outperforms direction/detailed-style instructions for VLN success rate (SR), and the SR decreases as the navigation length increases.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Embodied-AI Wheelchair Framework with Hands-free Interface and Manipulation</title>
      <link>/publication/22-embodied-ai-smc/</link>
      <pubDate>Sun, 02 Jan 2022 00:00:00 +0000</pubDate>
      <guid>/publication/22-embodied-ai-smc/</guid>
      <description>&lt;p&gt;Abstract:
Assistive robots can be found in hospitals and rehabilitation clinics, where they help patients maintain a positive disposition. Our proposed robotic mobility solution combines state of the art hardware and software to provide a safer, more independent, and more productive lifestyle for people with some of the most severe disabilities. New hardware includes, a retractable roof, manipulator arm, a hard backpack, a number of sensors that collect environmental data and processors that generate 3D maps for a hands-free human-machine interface. The proposed new system receives input from the user via head tracking or voice command, and displays information through augmented reality into the user’s field of view. The software algorithm will use a novel cycle of self-learning artificial intelligence that achieves autonomous navigation while avoiding collisions with stationary and dynamic objects. The prototype will be assembled and tested over the next three years and a publicly available version could be ready two years thereafter.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>End-To-End Training and Testing Gamification Framework to Learn Human Highway Driving</title>
      <link>/publication/22-end2end-driving-its/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>/publication/22-end2end-driving-its/</guid>
      <description>&lt;p&gt;Abstract:
The current autonomous stack is well modularized and consists of perception, decision making and control in a handcrafted framework. With the advances in artificial intelligence (AI) and computing resources, researchers have been pushing the development of end-to-end AI for autonomous driving, at least in problems of small searching space such as in highway scenarios, and more and more photorealistic simulation will be critical for efficient learning. In this research, we propose a novel game-based end-to-end learning and testing framework for autonomous vehicle highway driving, by learning from human driving skills. Firstly, we utilize the popular game Grand Theft Auto V (GTA V) to collect highway driving data with our proposed programmable labels. Then, an end-to-end architecture predicts the steering and throttle values that control the vehicle by the image of the game screen. The predicted control values are sent to the game via a virtual controller to keep the vehicle in lane and avoid collisions with other vehicles on the road. The proposed solution is validated in GTA V games, and the results demonstrate the effectiveness of this end-to-end gamification framework for learning human driving skills.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Advancing Self-Supervised Monocular Depth Learning with Sparse LiDAR</title>
      <link>/publication/21-depth-prediction-corl/</link>
      <pubDate>Mon, 20 Sep 2021 00:00:00 +0000</pubDate>
      <guid>/publication/21-depth-prediction-corl/</guid>
      <description>&lt;p&gt;Abstract:
Self-supervised monocular depth prediction provides a cost-effective solution to obtain the 3D location of each pixel. However, the existing approaches usually lead to unsatisfactory accuracy, which is critical for autonomous robots. In this paper, we propose FusionDepth, a novel two-stage network to advance the self-supervised monocular dense depth learning by leveraging low-cost sparse (e.g. 4-beam) LiDAR. Unlike the existing methods that use sparse LiDAR mainly in a manner of time-consuming iterative post-processing, our model fuses monocular image features and sparse LiDAR features to predict initial depth maps. Then, an efficient feed-forward refine network is further designed to correct the errors in these initial depth maps in pseudo-3D space with real-time performance. Extensive experiments show that our proposed model significantly outperforms all the state-of-the-art self-supervised methods, as well as the sparse-LiDAR-based methods on both self-supervised monocular depth prediction and completion tasks. With the accurate dense depth prediction, our model outperforms the state-of-the-art sparse-LiDAR-based method (Pseudo-LiDAR++) by more than 68% for the downstream task monocular 3D object detection on the KITTI Leaderboard.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multimodal Semi-Supervised Learning for 3D Objects</title>
      <link>/publication/21-m2cp-learning-bmvc/</link>
      <pubDate>Sun, 19 Sep 2021 00:00:00 +0000</pubDate>
      <guid>/publication/21-m2cp-learning-bmvc/</guid>
      <description>&lt;p&gt;Abstract:
In recent years, semi-supervised learning has been widely explored and shows excellent data efficiency for 2D data. There is an emerging need to improve data efficiency for 3D tasks due to the scarcity of labeled 3D data. This paper explores how the coherence of different modelities of 3D data (e.g. point cloud, image, and mesh) can be used to improve data efficiency for both 3D classification and retrieval tasks. We propose a novel multimodal semi-supervised learning framework by introducing instance-level consistency constraint and a novel multimodal contrastive prototype (M2CP) loss. The instance-level consistency enforces the network to generate consistent representations for multimodal data of the same object regardless of its modality. The M2CP maintains a multimodal prototype for each class and learns features with small intra-class variations by minimizing the feature distance of each object to its prototype while maximizing the distance to the others. Our proposed framework significantly outperforms all the state-of-the-art counterparts for both classification and retrieval tasks by a large margin on the modelNet10 and ModelNet40 datasets.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Nondestructive Evaluation of Terrain Using mmWave Radar Imaging</title>
      <link>/publication/21-mmwave-radar-sae/</link>
      <pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate>
      <guid>/publication/21-mmwave-radar-sae/</guid>
      <description>&lt;p&gt;Abstract:
Military ground vehicles operate in off-road environments traversing different terrains under various environmental conditions. There has been an increasing interest towards autonomous off-road vehicle navigation, leading to the needs of terrain traversability assessment through sensing. These methods utilized data-driven approaches on classical robotic perception sensing modalities (RGB cameras, Lidar, and depth cameras) positioned in front of ground vehicles in order to observe approaching terrain. Classical robotic sensing modalities, though effective for describing environment geometry and object detection and tracking, aren’t able to directly observe features related to compaction and moisture content which have significant effects on the moduli properties governing terrain mechanics. These methods then become very specialized to specific regions and environmental conditions which are inevitably subject to change. Radio wave-based sensing modes have been shown in studies to have success in observing different terrain surface and subsurface conditions such as compaction and moisture presence. We study the usability of emerging, portable and front mountable radar imaging sensors to provide real-time radio spectra information of the in-coming terrain area. In this study, we use a radar transceiver array operating in the 6.2-6.9 GHz spectral range to develop a radar image/soil moisture dataset, where beamforming is used to recover radar images of lab soil samples of various moisture content levels. The radar images are constructed at various distances from the soil surface and various spatial resolutions to support a local path planning scenario. Support vector machine (SVM) classifier and support vector regression (SVR) models are trained on the dataset and tested on lab data and in-field data. Classifier and regression model results indicate that normalized local radar image statistics are able to distinguish moisture levels at various distances and spatial resolutions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>3D Mapping and Stability Prediction for Autonomous Wheelchairs</title>
      <link>/publication/20-wheelchair-stability-cyber/</link>
      <pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/publication/20-wheelchair-stability-cyber/</guid>
      <description>&lt;p&gt;Abstract:
Autonomous wheelchairs can address a very large need in many populations by serving as the gateway to a much higher degree of independence and mobility capability. This is due to the fact that the big picture idea for autonomous wheelchairs integration into the transportation chain is to allow for individuals to be able to utilize the Intelligent wheelchair to reach the vehicle (regardless of terrain), mount into autonomous wheelchair that navigates to desired destination, and finally autonomous wheelchair dismounts. This will enable a higher degree of mobility for a handicapped population that experiences a large quantity of restrictions as a result of their circumstances. In order for this potential to be achieved numerous precautions must be integrated into the control system, such as stability maintenance. This paper focuses on mapping the environment through the use of a LiDAR sensor and predicting the stability of the given wheelchair. We utilize RTAB Mapping in combination with LiDAR odometry to construct a 3D map of the environment. Then Poisson reconstruction is deployed to convert the built 3D pointcloud into triangular mesh that allows for the norms to the surface to be calculated, which allows for stability prediction. This paper, not only outlines a novel pipeline but also deployed the pipeline on the recently released Intel RealSense L515 sensor and leverages its unique capabilities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Semantic Digital Surface Map Towards Collaborative Off-Road Vehicle Autonomy</title>
      <link>/publication/20-gvsets-mapping/</link>
      <pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/publication/20-gvsets-mapping/</guid>
      <description>&lt;p&gt;Abstract:
The fundamental aspect of unmanned ground vehicle (UGV) navigation, especially over off-road environments, are representations of terrain describing geometry, types, and traversability. One of the typical representations of the environment is digital surface models (DSMs) which efficiently encode geometric information. In this research, we propose a collaborative approach for UGV navigation through unmanned aerial vehicle (UAV) mapping to create semantic DSMs, by leveraging the UAV wide field of view and nadir perspective for map surveying. Semantic segmentation models for terrain recognition are affected by sensing modality as well as dataset availability. We explored and developed semantic segmentation deep convolutional neural networks (CNN) models to construct semantic DSMs. We further conducted a thorough quantitative and qualitative analysis regarding image modalities (between RGB, RGB+DSM and RG+DSM) and dataset availability effects on the performance of segmentation CNN models.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Modeling and Prediction of Rollover Stability for All-Terrain Vehicles</title>
      <link>/publication/20-gvsets-atv-stability/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/publication/20-gvsets-atv-stability/</guid>
      <description>&lt;p&gt;Abstract:
With the particular passage capability, all-terrain vehicle (ATV) has been widely used for off-road scenarios. In this research, we conduct a lateral sway stability analysis for the suspension mechanism of a general vehicle and establish a mathematical model of static and dynamic stability based on the maximum lateral sway angle and lateral sway acceleration, by considering the combined angular stiffness of independent suspension, angular stiffness of the lateral stabilizer bar and vertical stiffness of tires. 3D point cloud data of a terrain environment is collected using an RGB-Depth camera, and a triangular topography map is constructed. The results in ADAMS show that the proposed stability model can accurately predict the critical tipping state of the vehicle, and the method deployed for real-world terrain modeling and simulation analysis is generalizable for the stability assessment of the interaction between ATV and real-world terrain.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predicting Wheelchair Stability While Crossing a Curb Using RGB-Depth Vision</title>
      <link>/publication/20-wheelchair-curb-icchp/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <guid>/publication/20-wheelchair-curb-icchp/</guid>
      <description>&lt;p&gt;Abstract:
Handicapped individuals often rely heavily on various assistive technologies including wheelchairs and the purpose of these technologies is to enable greater levels of independence for the user. In the development of autonomous wheelchairs, it is imperative that the wheelchair maintains appropriate stability for the user in an outdoor urban environment. This paper proposes an RGB-Depth based perception algorithm for 3D mapping of the environment in addition to dynamic modeling of the wheelchair for stability analysis and prediction. We utilize RTAB Mapping in combination with Poisson Reconstruction that produced triangular mesh from which an accurate prediction of the stability of the wheelchair can be made based on the normals and the critical angle calculated from the dynamic model of the wheelchair.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Driver Drowsiness Behavior Detection and Analysis Using Vision-Based Multimodal Features for Driving Safety</title>
      <link>/publication/20-driver-drowsiness-sae/</link>
      <pubDate>Tue, 14 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/publication/20-driver-drowsiness-sae/</guid>
      <description>&lt;p&gt;Abstract:
Driving inattention caused by drowsiness has been a significant reason for vehicle crash accidents, and there is a critical need to augment driving safety by monitoring driver drowsiness behaviors. For real-time drowsy driving awareness, we propose a vision-based driver drowsiness monitoring system (DDMS) for driver drowsiness behavior recognition and analysis. First, an infrared camera is deployed in-vehicle to capture the driver’s facial and head information in naturalistic driving scenarios, in which the driver may or may not wear glasses or sunglasses. Second, we propose and design a multi-modal features representation approach based on facial landmarks, and head pose which is retrieved in a convolutional neural network (CNN) regression model. Finally, an extreme learning machine (ELM) model is proposed to fuse the facial landmark, recognition model and pose orientation for drowsiness detection. The DDMS gives promptly warning to the driver once a drowsiness event is detected. The proposed CNN and ELM models are trained in a drowsy driving dataset and are validated on public datasets and field tests. Comparing to the end-to-end CNN recognition model, the proposed multi-modal fusion with the ELM detection model allows faster and more accurate detection with minimal intervention. The experimental result demonstrates that DDMS is able to provide real-time and effective drowsy driving alerts under various light conditions to augment driving safety.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Neural Network based Visual Inspection with 3D Metric Measurement of Concrete Defects using Wall-climbing Robot</title>
      <link>/publication/19-visual-inspection-iros/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/publication/19-visual-inspection-iros/</guid>
      <description>&lt;p&gt;Abstract:
This paper presents a novel metric inspection robot system using a deep neural network to detect and measure surface flaws (i.e., crack and spalling) on concrete structures performed by a wall-climbing robot. The system consists of four modules: robotics data collection module to obtain RGB-D images and IMU measurement, visual-inertial SLAM module to generate pose coupled key-frames with depth information, InspectionNet module to classify each pixel into three classes (back-ground, crack and spalling), and 3D registration and map fusion module to register the flaw patch into registered 3D model overlaid and highlighted with detected flaws for spatial-contextual visualization. The system enables the metric model of each surface flaw patch with pixel-level accuracy and determines its location in 3D space that is significant for structural health assessment and monitoring. The InspectionNet achieves an average accuracy of 87.64% for crack and spalling inspection. We also demonstrate our InspectionNet is robust to view angle, scale and illumination variation. Finally, we design a metric voxel volume map to highlight the flaw in 3D model and provide location and metric information.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Semantic Metric 3D Reconstruction for Concrete Inspection</title>
      <link>/publication/18-metric-recon-cvprw/</link>
      <pubDate>Mon, 18 Jun 2018 00:00:00 +0000</pubDate>
      <guid>/publication/18-metric-recon-cvprw/</guid>
      <description>&lt;p&gt;Abstract:
In this paper, we exploit the concrete surface flaw inspection through the fusion of visual positioning and semantic segmentation approach. The fused inspection result is represented by a 3D metric map with a spatial area, width, and depth information, which shows the advantage over general inspection in image space without metric info. We also relieve the human labor with an automatic labeling approach. The system is composed of three hybrid parts: visual positioning to enable pose association, crack/spalling inspection using a deep neural network (pixel level), and a 3D random field filter for fusion to achieve a global 3D metric map. To improve the infrastructure inspection, we released a new data set for concrete crack and spalling segmentation which is built on CSSC dataset [27]. To leverage the effectiveness of the large-scale SLAM aided semantic inspection, we performed three field tests and one baseline test. Experimental results show that our proposed approach significantly improves the capability of 3D metric concrete inspection via deploying visual SLAM. Furthermore, we achieve an 82.4% MaxF1 score for crack detection and 88.64% MaxF1 score for spalling detection on the relabeled dataset.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Robotic System Towards Concrete Structure Spalling and Crack Database</title>
      <link>/publication/17-spalling-crack-db-robio/</link>
      <pubDate>Tue, 05 Dec 2017 00:00:00 +0000</pubDate>
      <guid>/publication/17-spalling-crack-db-robio/</guid>
      <description>&lt;p&gt;Abstract:
Concrete spalling and crack inspection is a labor intensive and routine task. However, it plays an important role in structure health monitoring (SHM) of civil infrastructures. Autonomous inspection with robots has been regarded as one of the best ways to reduce both error and cost. This paper presents an automated approach using Unmanned Aerial Vehicle(UAV) and towards a Concrete Structure Spalling and Crack database (CSSC), which is by far the first released database for deep learning inspection. We aim locate the spalling and crack regions to assist 3D registration and visualization. For deep inspection, we provide a complete procedure of data searching, labeling, training, and post processing. We further present a visual Simultaneously Localization and Mapping(SLAM) approach for localization and reconstruction. Comparative experiments and field tests are illustrated, results show that we can achieve an accuracy over 70% for field tests, and more than 93% accuracy with CSSC database.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Assistive Indoor Navigation System for the Visually Impaired in Multi-Floor Environments (Best Conference Paper Award)</title>
      <link>/publication/17-indoor-navi-cyber/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      <guid>/publication/17-indoor-navi-cyber/</guid>
      <description>&lt;p&gt;Abstract:
This paper presents an innovative wearable system to assist visually impaired people navigate indoors in real time. Our proposed system incorporates state-of-the-art handheld devices from Google&amp;rsquo;s Project Tango and integrates path planner and obstacle avoidance submodules, as well as human-computer interaction techniques, to provide assistance to the user. Our system preprocesses a priori knowledge of the environment extracted from CAD files and spatial information from Google&amp;rsquo;s Area Description Files. The system can then reallocate resources to navigation and human-computer interaction tasks during execution. The system is capable of exploring complex environments spanning multiple floors and has been tested and demonstrated in a variety of indoor environments.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CCNY Smart Cane</title>
      <link>/publication/17-smart-cane-cyber/</link>
      <pubDate>Mon, 31 Jul 2017 00:00:00 +0000</pubDate>
      <guid>/publication/17-smart-cane-cyber/</guid>
      <description>&lt;p&gt;Abstract:
This paper presents SmartCane - the CCNY Smart Cane system, a robotic white cane and mobile device navigation software for visually impaired people. The system includes software for Google Tango devices that utilizes simultaneous localization and mapping (SLAM) to plan a path and guide a visually impaired user to waypoints within indoor environments. A control panel is mounted on the standard white cane that enables visually impaired users to communicate with the navigation software and is additionally used to provide navigation instructions via haptic feedback. Based on the motion-tracking and localization capabilities of the Google Tango, the SmartCane is able to generate a safe path to the destination waypoint indicated by the user.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rise-Rover: A Wall-Climbing Robot with High Reliability and Load-Carrying Capacity (Industrial Robot Innovation Award)</title>
      <link>/publication/15-wall-climbing-clawar/</link>
      <pubDate>Mon, 31 Jul 2017 00:00:00 +0000</pubDate>
      <guid>/publication/15-wall-climbing-clawar/</guid>
      <description>&lt;p&gt;Abstract:
This paper presents Rise-Rover, a new generation wall-climbing robot with high reliability and load-carrying capacity on vertical surfaces for Non-Destructive Testing (NDT) of concrete and steel infrastructure. One Rise-Rover drivetrain module can operate on both smooth and rough vertical/inclined surfaces independently. The on-board electronics and PID controller monitor the pressure reading and adjust impeller speed to provide stable suction force for the wall-climbing robot. The use of duct fan and tether further increase the operation reliability. Rise-Rover is remotely controlled by Android smartphone via Wifi, and the User Interface (UI) provides good usability and convenience. The experimental test verified the good performance of the Rise-Rover prototype.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Guided Text Spotting for Assistive Blind Navigation in Unfamiliar Indoor Environments</title>
      <link>/publication/16-text-recog-isvc/</link>
      <pubDate>Sat, 10 Dec 2016 00:00:00 +0000</pubDate>
      <guid>/publication/16-text-recog-isvc/</guid>
      <description>&lt;p&gt;Abstract:
Scene text in indoor environments usually preserves and communicates important contextual information which can significantly enhance the independent travel of blind and visually impaired people. In this paper, we present an assistive text spotting navigation system based on an RGB-D mobile device for blind or severely visually impaired people. Specifically, a novel spatial-temporal text localization algorithm is proposed to localize and prune text regions, by integrating stroke-specific features with a subsequent text tracking process. The density of extracted text-specific feature points serves as an efficient text indicator to guide the user closer to text-likely regions for better recognition performance. Next, detected text regions are binarized and recognized by off-the-shelf optical character recognition methods. Significant non-text indicator signage can also be matched to provide additional environment information. Both recognized results are then transferred to speech feedback for user interaction. Our proposed video text localization approach is quantitatively evaluated on the ICDAR 2013 dataset, and the experimental results demonstrate the effectiveness of our proposed method.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Demo: Assisting Visually Impaired People Navigate Indoors</title>
      <link>/publication/16-demo-navi-ijcai/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      <guid>/publication/16-demo-navi-ijcai/</guid>
      <description>&lt;p&gt;Abstract:
Research in Artificial Intelligence, Robotics and Computer Vision has recently made great strides in improving indoor localization. Publicly available technology now allows for indoor localization with very small margins of error. In this demo, we show a system that uses state-of the-art technology to assist visually impaired people navigate indoors. Our system takes advantage of spatial representations from CAD files, or floor plan images, to extract valuable information that later can be used to improve navigation and human-computer interaction. Using depth information, our system is capable of detecting obstacles and guiding the user to avoid them.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ISANA: Wearable Context-Aware Indoor Assistive Navigation with Obstacle Avoidance for the Blind</title>
      <link>/publication/16-isana-navi-eccvw/</link>
      <pubDate>Thu, 03 Nov 2016 00:00:00 +0000</pubDate>
      <guid>/publication/16-isana-navi-eccvw/</guid>
      <description>&lt;p&gt;Abstract:
This paper presents a novel mobile wearable context-aware indoor maps and navigation system with obstacle avoidance for the blind. The system includes an indoor map editor and an App on Tango devices with multiple modules. The indoor map editor parses spatial semantic information from a building architectural model, and represents it as a high-level semantic map to support context awareness. An obstacle avoidance module detects objects in front using a depth sensor. Based on the ego-motion tracking within the Tango, localization alignment on the semantic map, and obstacle detection, the system automatically generates a safe path to a desired destination. A speech-audio interface delivers user input, guidance and alert cues in real-time using a priority-based mechanism to reduce the user’s cognitive load. Field tests involving blindfolded and blind subjects demonstrate that the proposed prototype performs context-aware and safety indoor assistive navigation effectively.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A SLAM based Semantic Indoor Navigation System for Visually Impaired Users</title>
      <link>/publication/15-assistive-navi-smc/</link>
      <pubDate>Fri, 09 Oct 2015 00:00:00 +0000</pubDate>
      <guid>/publication/15-assistive-navi-smc/</guid>
      <description>&lt;p&gt;Abstract:
This paper proposes a novel assistive navigation system based on simultaneous localization and mapping (SLAM) and semantic path planning to help visually impaired users navigate in indoor environments. The system integrates multiple wearable sensors and feedback devices including a RGB-D sensor and an inertial measurement unit (IMU) on the waist, a head mounted camera, a microphone and an earplug/speaker. We develop a visual odometry algorithm based on RGB-D data to estimate the user&amp;rsquo;s position and orientation, and refine the orientation error using the IMU. We employ the head mounted camera to recognize the door numbers and the RGB-D sensor to detect major landmarks such as corridor corners. By matching the detected landmarks against the corresponding features on the digitalized floor map, the system localizes the user, and provides verbal instruction to guide the user to the desired destination. The software modules of our system are implemented in Robotics Operating System (ROS). The prototype of the proposed assistive navigation system is evaluated by blindfolded sight persons. The field tests confirm the feasibility of the proposed algorithms and the system prototype.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robotic Impact-Echo Non-Destructive Evaluation based on FFT and SVM</title>
      <link>/publication/14-impact-echo-wcica/</link>
      <pubDate>Sun, 29 Jun 2014 00:00:00 +0000</pubDate>
      <guid>/publication/14-impact-echo-wcica/</guid>
      <description>&lt;p&gt;Abstract:
Previous research has shown that the impact-echo emission signals contain information about the flaws of structural integrity and deterioration levels of concrete bridges. This paper presents a method of using the mobile robot equipped with an impact-echo Non-Destructive Evaluation (NDE) device to autonomously collect data, perform automatic classification and 3D visualization of the detected flaws. This method is based on Power Spectral Density (PSD) analysis for the Fast Fourier Transform (FFT) of the impact-echo signals, and Support Vector Machine (SVM) classification. Therefore, health condition of concrete bridge decks can be automatically recorded, analyzed and visualized in 3D.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Autonomous Vehicle Following Approach-The Virtual Flexible Bar Model</title>
      <link>/publication/13-vehicle-following/</link>
      <pubDate>Tue, 08 Jan 2013 00:00:00 +0000</pubDate>
      <guid>/publication/13-vehicle-following/</guid>
      <description>&lt;p&gt;Abstract:
An improved virtual flexible bar algorithm which aims at improving the trajectory tracking accuracy of automatic vehicle following system is proposed in this paper. To achieve successful vehicle following, the algorithm mainly consists two parts: the flexible bar which can fit the leader vehicle&amp;rsquo;s trajectory and the force acting in the flexible bar which pull the follower vehicle to track the leader&amp;rsquo;s trajectory besides adjusting the velocity and acceleration of the follower according to the motion state of the leader. In order to maintain the smooth motion of the follower and make sure that the follower could carry out the commands transmitted from the controller accurately, an adaptive trajectory tracking control is also implemented. The simulated experimental results validate that the follower is able to trail the trajectories of the leader vehicle keep the error within an satisfied limits meanwhile maintaining a safe following distance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tension Distribution Algorithm of a 7-DOF Cable-Driven Robotic Arm Based on Dynamic Minimum Pre-Tightening Force</title>
      <link>/publication/11-robotic-arm-robio/</link>
      <pubDate>Wed, 07 Dec 2011 00:00:00 +0000</pubDate>
      <guid>/publication/11-robotic-arm-robio/</guid>
      <description>&lt;p&gt;Abstract:
From shoulder to wrist, a person&amp;rsquo;s arm has a total of seven degrees of freedom (DOFs), which is the minimum number of DOF for the robot that needs to avoid obstacles and internal singularity. Compared to the traditional 7-DOF robot, the 7-DOF cable-driven robotic arm (CDRA) similar to human muscles parallel drive mode is the hybrid structure of serial-parallel, which possesses a number of promising advantages, such as simple and light-weight mechanical structure, high-loading capacity, and large reachable workspace. Since the cable can only generate tension and cannot stand pressure, cable-driven mechanism must be a redundant drive structure. Both the shoulder and wrist joint are redundant drive mechanism, and the tension of four cables has multiple solutions in the same posture. Hence, iteration Newton-Euler method is adopted to conduct dynamic analysis. Cable tension distribution algorithm based on null space method by solving Pseudo-inverse matrix is proposed, and in order to keep the cable tensional and also reduce energy consumption, the index of dynamic minimum pre-tightening force is originally proposed to realtime adjust the driving force of each cable. To show the accuracy and effectiveness of the proposed cable tension distribution algorithm, several simulation results are illustrated. These lay a good theory foundation for further research on effective control and performance improvement of the CDRA.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Construction and Realization of a 3D Perceptual System Based on 2D Laser Radar</title>
      <link>/publication/08-perceptual-sys-iciea/</link>
      <pubDate>Thu, 05 Jun 2008 00:00:00 +0000</pubDate>
      <guid>/publication/08-perceptual-sys-iciea/</guid>
      <description>&lt;p&gt;Abstract:
This paper presents a 3D perceptual system based on 2D laser radar LMS291. The proposed system utilizes intelligent module to add a degree of freedom to LMS291. A new method of synchronization between LMS291 and intelligent module was proposed. We added Visual C++ multi-media timer to the timestamp method, the purpose we use this method is to change the level resolution conveniently, and control the number of the points we acquired, because in some condition we need speedy scanning to rebuild the reconstructed image of the terrain quickly. And we use Matlab to reconstruct the 2D and 3D images. To repair the singular points we selected absolute mean value method. The experiment shows that absolute mean value method works more accurately and effectively And this system we designed can be used for image reconstruction in unstructured environment by experiments.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>3D Reconstruction Embedded System Based on Laser Scanner for Mobile Robot</title>
      <link>/publication/08-3d-recon-iciea/</link>
      <pubDate>Tue, 03 Jun 2008 00:00:00 +0000</pubDate>
      <guid>/publication/08-3d-recon-iciea/</guid>
      <description>&lt;p&gt;Abstract:
For 3D reconstruction technology based on laser ranging for mobile robot, this paper presents the design of real-time embedded system for 3D data processing. Firstly, this paper introduces the home and abroad research situation of laser ranging for mobile robot, and analysis the method of 3D data acquisition with 2D laser scanner and ID driving motor module. Secondly, this paper introduces the hardware design and system building of multi-module embedded system, which consists of DSP data acquisition module, FPGA data processing module and ARM control. The ARM system is the main control system and realizes 3D reconstruction function. CAN bus communication is selected between these modules. Thirdly, this paper introduces the design of communication programming under ARM control system. At last, this paper presents the 3D reconstruction method with laser scanner and driving motor by Mesa3D based on RTLinux system.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

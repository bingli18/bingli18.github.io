<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Johnell Brooks | AutoAI Lab</title>
    <link>/author/johnell-brooks/</link>
      <atom:link href="/author/johnell-brooks/index.xml" rel="self" type="application/rss+xml" />
    <description>Johnell Brooks</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 03 Jan 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/logo_hufbce6705a566d43a9610d78203895d17_430824_300x300_fit_lanczos_2.png</url>
      <title>Johnell Brooks</title>
      <link>/author/johnell-brooks/</link>
    </image>
    
    <item>
      <title>SeeWay: Vision-Language Assistive Navigation for the Visually Impaired</title>
      <link>/publication/22-seeway-smc/</link>
      <pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate>
      <guid>/publication/22-seeway-smc/</guid>
      <description>&lt;p&gt;Abstract:
Assistive navigation for blind or visually impaired (BVI) individuals is of significance to extend their mobility and safety in traveling, enhancing their employment opportunities and fostering personal fulfillment. Conventional research is mainly based on robotic navigation approaches through localization, mapping, and path planning frameworks. They require heavy manual annotation of semantic information in maps and its alignment with sensor mapping. Inspired by the fact that we human beings naturally rely on language instruction inquiry and visual scene understanding to navigate in an unfamiliar environment, this paper proposes a novel vision-language model-based approach for BVI navigation. It does not need heavy-labeled indoor maps and provides a Safe and Efficient E-Wayfinding (SeeWay) assistive solution for BVI individuals. The system consists of a scene-graph map construction module, a navigation path generation module for global path inference by vision-language navigation (VLN), and a navigation with obstacle avoidance module for real-time local navigation. The SeeWay system was deployed on portable iPhone devices with cloud computing assistance for the VLN model inference. The field tests show the effectiveness of the VLN global path finding and local path re-planning. Experiments and quantitative results reveal that heuristic-style instruction outperforms direction/detailed-style instructions for VLN success rate (SR), and the SR decreases as the navigation length increases.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

[{"authors":["admin"],"categories":null,"content":"I am an Assistant Professor of Automotive Engineering at Clemson University International Center for Automotive Research (CU-ICAR) since 2018, directing AutoAI Lab research group.\nMy team is focusing on Autonomous AI research especially robotic Perception \u0026amp; Intelligence in interactive, dynamic, and uncertain environments, including topics such as sensing, visual perception/mapping, deep/machine learning, and artificial intelligence (AI) for robotics. We are also developing assistive and assistance technologies of navigation and safety aid to helping people with special needs.\nPrior to joining Clemson, I earned a Ph.D. degree in Electrical Engineering at The City College (CCNY), The City University of New York (CUNY). I also had industrial R\u0026amp;D experiences at China Academy of Telecommunications Technology, IBM and HERE North America LLC that builds maps and location platform enabling self-driving vehicles.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/bing-li-ph.d./","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/bing-li-ph.d./","section":"authors","summary":"I am an Assistant Professor of Automotive Engineering at Clemson University International Center for Automotive Research (CU-ICAR) since 2018, directing AutoAI Lab research group.\nMy team is focusing on Autonomous AI research especially robotic Perception \u0026amp; Intelligence in interactive, dynamic, and uncertain environments, including topics such as sensing, visual perception/mapping, deep/machine learning, and artificial intelligence (AI) for robotics.","tags":null,"title":"Bing Li, Ph.D.","type":"authors"},{"authors":["Jesse Leaman","Hung La","Bing Li"],"categories":null,"content":"Abstract: Disability knows no boarders, so the development of assistive technology is an international effort. This review is a follow up to our previous comprehensive review (Leaman 2017) and a recent mini-review (Sivakanthan 2022). The transition from Power Wheelchair to Robotic Wheelchair (RW) with various operating modes like, Docking, Guide Following, and Path Planning for Autonomous Navigation, has become an attainable goal. Thanks to the revolution in Aerial Drones for the consumer market, many of the necessary algorithms for the RW software have already been developed. The challenge is to put forward a system that will be embraced by the population they are meant to serve. The Human Computer Interface (HCI) will have to be interactive, with all input and output methods depending on the user’s physical capabilities. In addition, all operating modes have to be customizable, based on the preferences of each user. Variables like maximum speed, and minimum distance to obstacles, are input conditions for many operating modes that will impact the user’s experience. The HCI should be able to explain its decisions in order to increase its trustworthiness over time. This may be in the form of verbal communication or visual feedback projected into the user’s field of view like augmented reality.Given the commitment of the international research community, and the growing demand, a commercially viable RW should become reality within the next decade. This will have a positive impact on millions of seniors and people with disabilities, their caregivers, and the governments paying for long-term care programs. The RW will pay for itself by reducing the number of caregiver hours needed to provide the same level of independence. The RW should even positively impact the economy since some users will have the confidence to return to work, and many will be able to participate in social events.\n","date":1717200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717200000,"objectID":"da1a7072220ec8c0a63c56ec3a594688","permalink":"/publication/24-aichair-ahfe/","publishdate":"2024-06-01T00:00:00Z","relpermalink":"/publication/24-aichair-ahfe/","section":"publication","summary":"Abstract: Disability knows no boarders, so the development of assistive technology is an international effort. This review is a follow up to our previous comprehensive review (Leaman 2017) and a recent mini-review (Sivakanthan 2022).","tags":null,"title":"An Update on International Robotic Wheelchair Development","type":"publication"},{"authors":["Zhimin Chen","Longlong Jing","Yingwei Li","Bing Li"],"categories":null,"content":"Abstract: Foundation models have made significant strides in 2D and language tasks such as image segmentation, object detection, and visual-language understanding. Nevertheless, their potential to enhance 3D scene representation learning remains largely untapped due to the domain gap. In this paper, we propose an innovative methodology Bridge3D to address this gap, pre-training 3D models using features, semantic masks, and captions sourced from foundation models. Specifically, our approach utilizes semantic masks from these models to guide the masking and reconstruction process in the masked autoencoder. This strategy enables the network to concentrate more on foreground objects, thereby enhancing 3D representation learning. Additionally, we bridge the 3D-text gap at the scene level by harnessing image captioning foundation models. To further facilitate knowledge distillation from well-learned 2D and text representations to the 3D model, we introduce a novel method that employs foundation models to generate highly accurate object-level masks and semantic text information at the object level. Our approach notably outshines state-of-the-art methods in 3D object detection and semantic segmentation tasks. For instance, on the ScanNet dataset, our method surpasses the previous state-of-the-art method, PiMAE, by a significant margin of 5.3%.\n","date":1695254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695254400,"objectID":"a9629b90af2d581005a1f89e9f314d37","permalink":"/publication/23-domaingap/","publishdate":"2023-09-21T00:00:00Z","relpermalink":"/publication/23-domaingap/","section":"publication","summary":"Abstract: Foundation models have made significant strides in 2D and language tasks such as image segmentation, object detection, and visual-language understanding. Nevertheless, their potential to enhance 3D scene representation learning remains largely untapped due to the domain gap.","tags":["selected","Visual Computing","Deep Learning"],"title":"Bridging the Domain Gap: Self-Supervised 3D Scene Understanding with Foundation Models","type":"publication"},{"authors":["Ziyue Feng","Liang Yang","Pengsheng Guo","Bing Li"],"categories":null,"content":"Abstract: Recent advances in neural reconstruction using posed image sequences have made remarkable progress. However, due to the lack of depth information, existing volumetric-based techniques simply duplicate 2D image features of the object surface along the entire camera ray. We contend this duplication introduces noise in empty and occluded spaces, posing challenges for producing high-quality 3D geometry. Drawing inspiration from traditional multi-view stereo methods, we propose an end-to-end 3D neural reconstruction framework CVRecon, designed to exploit the rich geometric embedding in the cost volumes to facilitate 3D geometric feature learning. Furthermore, we present Ray-contextual Compensated Cost Volume, a novel 3D geometric feature representation that encodes view-dependent information with improved integrity and robustness. Through comprehensive experiments, we demonstrate that our approach significantly improves the reconstruction quality in various metrics and recovers clear fine details of the 3D geometries. Our extensive ablation studies provide insights into the development of effective 3D geometric feature learning schemes. The code will be made publicly available.\n","date":1689379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689379200,"objectID":"e06d361dcdf444560c3bbd7ffb4b2f17","permalink":"/publication/23-rethinking3d-iccv/","publishdate":"2023-07-15T00:00:00Z","relpermalink":"/publication/23-rethinking3d-iccv/","section":"publication","summary":"Abstract: Recent advances in neural reconstruction using posed image sequences have made remarkable progress. However, due to the lack of depth information, existing volumetric-based techniques simply duplicate 2D image features of the object surface along the entire camera ray.","tags":["selected","Visual Computing","Deep Learning","Autonomous Driving"],"title":"Rethinking 3D Geometric Feature Learning for Neural Reconstruction","type":"publication"},{"authors":["Ziyue Feng","Liang Yang","Longlong Jing","Haiyan Wang","Yingli Tian","Bing Li"],"categories":null,"content":"Abstract: Conventional self-supervised monocular depth prediction methods are based on a static environment assumption, which leads to accuracy degradation in dynamic scenes due to the mismatch and occlusion problems introduced by object motions. Existing dynamic-object-focused methods only partially solved the mismatch problem at the training loss level. In this paper, we accordingly propose a novel multi-frame monocular depth prediction method to solve these problems at both the prediction and supervision loss levels. Our method, called DynamicDepth, is a new framework trained via a self-supervised cycle consistent learning scheme. A Dynamic Object Motion Disentanglement (DOMD) module is proposed to disentangle object motions to solve the mismatch problem. Moreover, novel occlusion-aware Cost Volume and Re-projection Loss are designed to alleviate the occlusion effects of object motions. Extensive analyses and experiments on the Cityscapes and KITTI datasets show that our method significantly outperforms the state-of-the-art monocular depth prediction methods, especially in the areas of dynamic objects. Our code will be made publicly available.\n","date":1689292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689292800,"objectID":"42ec8c77ffc8fe49d9c6e2bdc25d72c3","permalink":"/publication/22-dynamicdepth/","publishdate":"2023-07-14T00:00:00Z","relpermalink":"/publication/22-dynamicdepth/","section":"publication","summary":"Abstract: Conventional self-supervised monocular depth prediction methods are based on a static environment assumption, which leads to accuracy degradation in dynamic scenes due to the mismatch and occlusion problems introduced by object motions.","tags":["selected","Visual Computing","Deep Learning","Autonomous Driving"],"title":"Disentangling Object Motion and Occlusion for Unsupervised Multi-frame Monocular Depth","type":"publication"},{"authors":["Ziyue Feng","Longlong Jing","Peng Yin","Yingli Tian","Bing Li"],"categories":null,"content":"Abstract: Self-supervised monocular depth prediction provides a cost-effective solution to obtain the 3D location of each pixel. However, the existing approaches usually lead to unsatisfactory accuracy, which is critical for autonomous robots. In this paper, we propose FusionDepth, a novel two-stage network to advance the self-supervised monocular dense depth learning by leveraging low-cost sparse (e.g. 4-beam) LiDAR. Unlike the existing methods that use sparse LiDAR mainly in a manner of time-consuming iterative post-processing, our model fuses monocular image features and sparse LiDAR features to predict initial depth maps. Then, an efficient feed-forward refine network is further designed to correct the errors in these initial depth maps in pseudo-3D space with real-time performance. Extensive experiments show that our proposed model significantly outperforms all the state-of-the-art self-supervised methods, as well as the sparse-LiDAR-based methods on both self-supervised monocular depth prediction and completion tasks. With the accurate dense depth prediction, our model outperforms the state-of-the-art sparse-LiDAR-based method (Pseudo-LiDAR++) by more than 68% for the downstream task monocular 3D object detection on the KITTI Leaderboard.\n","date":1689206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689206400,"objectID":"81e5bf286853c44e7f96d78a9034e608","permalink":"/publication/21-depth-prediction-corl/","publishdate":"2023-07-13T00:00:00Z","relpermalink":"/publication/21-depth-prediction-corl/","section":"publication","summary":"Abstract: Self-supervised monocular depth prediction provides a cost-effective solution to obtain the 3D location of each pixel. However, the existing approaches usually lead to unsatisfactory accuracy, which is critical for autonomous robots.","tags":["selected","Visual Computing","Deep Learning","Autonomous Driving"],"title":"Advancing Self-Supervised Monocular Depth Learning with Sparse LiDAR","type":"publication"},{"authors":["Bangquan Xie","Liang Yang","Zongming Yang","Ailin Wei","Xiaoxiong Weng","Bing Li"],"categories":null,"content":"Abstract: This research proposes a novel semi-supervised learning framework FourStr (Four-Stream formed by two two-stream models) that focuses on the improvement of fusion and labeling efficiency for 3D multi-sensor detector. FourStr adopts a multi-sensor single-stage detector named adaptive fusion network (AFNet) as the backbone and trains it through the semi-supervision learning (SSL) strategy Stereo Fusion. Note that multi-sensor AFNet and SSL Stereo Fusion can benefit each other. On the one hand, the Four-stream composed of two AFNets naturally provides rich inputs and large models for SSL Stereo Fusion. While other SSL works have to use massive augmentation to obtain rich inputs, and deepen and widen the network for large models. On the other hand, by the novel three fusion stages and Loss Pruning, Stereo Fusion improves the fusion and labeling efficiency for AFNet. Finally, extensive experiments demonstrate that FourStr performs excellently on outdoor dataset (KITTI and Waymo Open Dataset) and indoor dataset (SUN RGB-D), especially for the small contour objects. And compared to the fully-supervised methods, FourStr achieves similar accuracy with only 2% labeled data on KITTI (or with 50% labeled data on SUN RGB-D).\n","date":1678406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678406400,"objectID":"febeead2f527fa60df06f806a49b4f4a","permalink":"/publication/23-fourstr-icra/","publishdate":"2023-03-10T00:00:00Z","relpermalink":"/publication/23-fourstr-icra/","section":"publication","summary":"Abstract: This research proposes a novel semi-supervised learning framework FourStr (Four-Stream formed by two two-stream models) that focuses on the improvement of fusion and labeling efficiency for 3D multi-sensor detector. FourStr adopts a multi-sensor single-stage detector named adaptive fusion network (AFNet) as the backbone and trains it through the semi-supervision learning (SSL) strategy Stereo Fusion.","tags":["selected","Robotics","Visual Computing","Deep Learning"],"title":"FourStr: When Multi-sensor Fusion Meets Semi-supervised Learning","type":"publication"},{"authors":["Zhimin Chen","Longlong Jing","Liang Yang","Yingwei Li","Bing Li"],"categories":null,"content":"Abstract: Recent state-of-the-art method FlexMatch firstly demonstrated that correctly estimating learning status is crucial for semi-supervised learning (SSL). However, the estimation method proposed by FlexMatch does not take into account imbalanced data, which is the common case for 3D semi-supervised learning. To address this problem, we practically demonstrate that unlabeled data class-level confidence can represent the learning status in the 3D imbalanced dataset. Based on this finding, we present a novel class-level confidence based 3D SSL method. Firstly, a dynamic thresholding strategy is proposed to utilize more unlabeled data, especially for low learning status classes. Then, a re-sampling strategy is designed to avoid biasing toward high learning status classes, which dynamically changes the sampling probability of each class. To show the effectiveness of our method in 3D SSL tasks, we conduct extensive experiments on 3D SSL classification and detection tasks. Our method significantly outperforms state-of-the-art counterparts for both 3D SSL classification and detection tasks in all datasets.\n","date":1676851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676851200,"objectID":"31df92380ff1879e5c99cd885d8add37","permalink":"/publication/23-3d-semi-learning-wacv/","publishdate":"2023-02-20T00:00:00Z","relpermalink":"/publication/23-3d-semi-learning-wacv/","section":"publication","summary":"Abstract: Recent state-of-the-art method FlexMatch firstly demonstrated that correctly estimating learning status is crucial for semi-supervised learning (SSL). However, the estimation method proposed by FlexMatch does not take into account imbalanced data, which is the common case for 3D semi-supervised learning.","tags":["selected","Visual Computing","Deep Learning"],"title":"Class-Level Confidence Based 3D Semi-Supervised Learning","type":"publication"},{"authors":["Liang Yang","Bing Li","Jinglun Feng","Guoyong Yang","Yong Chang","Biao Jiang","Jizhong Xiao"],"categories":null,"content":"Abstract: Human-made concrete structures require cutting-edge inspection tools to ensure the quality of the construction to meet the applicable building codes and to maintain the sustainability of the aging infrastructure. This paper introduces a wall-climbing robot for metric concrete inspection that can reach difficult-to-access locations with a close-up view for visual data collection and real-time flaws detection and localization. The wall-climbing robot is able to detect concrete surface flaws (i.e., cracks and spalls) and produce a defect-highlighted 3D model with extracted location clues and metric measurements. The system encompasses four modules, including a data collection module to capture RGB-D frames and inertial measurement unit data, a visual–inertial navigation system module to generate pose-coupled keyframes, a deep neural network module (namely InspectionNet) to classify each pixel into three classes (background, crack, and spall), and a semantic reconstruction module to integrate per-frame measurement into a global volumetric model with defects highlighted. We found that commercial RGB-D camera output depth is noisy with holes, and a Gussian-Bilateral filter for depth completion is introduced to inpaint the depth image. The method achieves the state-of-the-art depth completion accuracy even with large holes. Based on the semantic mesh, we introduce a coherent defect metric evaluation approach to compute the metric measurement of crack and spall area (e.g., length, width, area, and depth). Field experiments on a concrete bridge demonstrate that our wall-climbing robot is able to operate on a rough surface and can cross over shallow gaps. The robot is capable to detect and measure surface flaws under low illuminated environments and texture-less environments. Besides the robot system, we create the first publicly accessible concrete structure spalls and cracks data set that includes 820 labeled images and over 10,000 field-collected images and release it to the research community.\n","date":1671062400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671062400,"objectID":"6da4e69332e4f26a5fa9921ddcab6acb","permalink":"/publication/22-wall-climbing-jfr/","publishdate":"2022-12-15T00:00:00Z","relpermalink":"/publication/22-wall-climbing-jfr/","section":"publication","summary":"Abstract: Human-made concrete structures require cutting-edge inspection tools to ensure the quality of the construction to meet the applicable building codes and to maintain the sustainability of the aging infrastructure. This paper introduces a wall-climbing robot for metric concrete inspection that can reach difficult-to-access locations with a close-up view for visual data collection and real-time flaws detection and localization.","tags":["selected","Robotics","Visual Computing","Sensing","Deep Learning"],"title":"Automated Wall-Climbing Robot for Concrete Construction Inspection","type":"publication"},{"authors":["Bangquan Xie","Liang Yang","Zongming Yang","Ailin Wei","Xiaoxiong Weng","Bing Li"],"categories":null,"content":"Abstract: The feature pyramid, which is a vital component of the convolutional neural networks, plays a significant role in several perception tasks, including object detection for autonomous driving. However, how to better fuse multi-level and multi-sensor feature pyramids is still a significant challenge, especially for object detection. This paper presents a FocusTR (Focusing on the valuable features by multiple Transformers), which is a simple yet effective architecture, to fuse feature pyramid for the single-stream 2D detector and two-stream 3D detector. Specifically, FocusTR encompasses several novel self-attention mechanisms, including the spatial-wise boxAlign attention (SB) for low-level spatial locations, context-wise affinity attention (CA) for high-level context information, and level-wise attention for the multi-level feature. To alleviate self-attention’s computational complexity and slow training convergence, FocusTR introduces a low and high-level fusion (LHF) to reduce the computational parameters, and the Pre-LN to accelerate the training convergence.\n","date":1670630400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670630400,"objectID":"14cf62d073c73567f7c8e9e4c168adca","permalink":"/publication/22-focustr-iros/","publishdate":"2022-12-10T00:00:00Z","relpermalink":"/publication/22-focustr-iros/","section":"publication","summary":"Abstract: The feature pyramid, which is a vital component of the convolutional neural networks, plays a significant role in several perception tasks, including object detection for autonomous driving. However, how to better fuse multi-level and multi-sensor feature pyramids is still a significant challenge, especially for object detection.","tags":["Robotics","Visual Computing","Deep Learning"],"title":"FocusTR: Focusing on Valuable Feature by Multiple Transformers for Fusing Feature Pyramid on Object Detection","type":"publication"},{"authors":["Bangquan Xie","Zongming Yang","Liang Yang","Ruifa Luo","Jun Lu","Ailin Wei","Xiaoxiong Weng","Bing Li"],"categories":null,"content":"Abstract: Current learning-based 3-D object detection accuracy is heavily impacted by the annotation quality. It is still a challenge to expect an overall high detection accuracy for all classes under different scenarios given the dataset sparsity. To mitigate this challenge, this article proposes a novel method called semi-supervised learning and progressive distillation (SPD), which uses semi-supervised learning (SSL) and knowledge distillation to improve label efficiency. The SPD uses two big backbones to hand the unlabeled/labeled input data augmented by the periodic IO augmentation (PA). Then the backbones are compressed using progressive distillation (PD). Precisely, PA periodically shifts the data augmentation operations between the input and output of the big backbone, aiming to improve the network’s generalization of the unseen and unlabeled data. Using the big backbone can benefit from large-scale augmented data better than the small one. And two backbones are trained by the data scale and ratio-sensitive loss (data-loss). It solves the over-flat caused by the large-scale unlabeled data from PA and helps the big backbone prevent overfitting on the limited-scale labeled data. Hence, using the PA and data loss during SSL training dramatically improves the label efficiency. Next, the trained big backbone set as the teacher CNN is progressively distilled to obtain a small student model, referenced as PD. PD mitigates the problem that student CNN performance degrades when the gap between the student and the teacher is oversized. Extensive experiments are conducted on the indoor datasets SUN RGB-D and ScanNetV2 and outdoor dataset KITTI. Using only 50% labeled data and a 27% smaller model size, SPD performs 0.32 higher than the fully supervised VoteNetqi2019deep which is adopted as our backbone. Besides, using only 2% labeled data, compared to the other fully supervised backbone PV-RCNNshi2020pv, SPD accomplishes a similar accuracy (84.1 and 84.83) and 30% less inference time.\n","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"1d13a720ec7f270821a5652555cebc20","permalink":"/publication/22-spd-tnnls/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/22-spd-tnnls/","section":"publication","summary":"Abstract: Current learning-based 3-D object detection accuracy is heavily impacted by the annotation quality. It is still a challenge to expect an overall high detection accuracy for all classes under different scenarios given the dataset sparsity.","tags":["Robotics","Visual Computing","Deep Learning"],"title":"SPD: Semi-Supervised Learning and Progressive Distillation for 3-D Detection","type":"publication"},{"authors":["Bangquan Xie","Zongming Yang","Liang Yang","Ailin Wei","Xiaoxiong Weng","Bing Li"],"categories":null,"content":"Abstract: Recently significant progress has been made in 3D detection. However, it is still challenging to detect small contour objects under complex scenes. This paper proposes a novel Attention-based Multi-phase Multi-task Fusion (AMMF) that uses point-level, RoI-level, and multi-task fusions to complement the disadvantages of LiDAR and camera, to solve this challenge. First, at the feature extraction phase, AMMF uses the Low and High-level Fusion with Matching Attention (LHF-MA) and efficient FPN (eFPN) to perform point-level fusion for cross sensors and single sensor, respectively. Instead of merging each level and using expensive 3D CNN like other methods, LHF-MA fuses low-level spatial location and high-level contextual feature of 2D CNN customized feature extractors and ignores the fusion of middle levels, reducing the computational cost. Then, at the proposal generation phase, Progressive Proposal Fusion (PPF) with learned attention map is used to perform coarse-to-fine RoI-level fusion, instead of only combining coarse-grained features at high-level of network. PPF using progressively increasing IoU thresholds could avoid overfitting and improve the performance. Note that the matching attentions and learned attention maps are utilized to weigh the priority of different sensors. Moreover, to solve the sparseness of point-wise fusion between LiDAR BEV and RGB image, AMMF uses multi-task fusion that generates pseudo-LiDAR from camera by depth estimation task, to guide this point-wise fusion. Finally, AMMF performs excellently for detecting small contour objects like pedestrians, cyclists, and distant cars. On the KITTI, AMMF finishes 3.62% improvements in the moderate instance for pedestrians. It achieves a 2.21% improvement in the \u0026gt;50 instance of LEVEL-2 level for vehicle on the Waymo Open Dataset. And AMMF is further verified on our customized dataset consisting of challenging scenarios like strong illumination and heavy shadow cases.\n","date":1669766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669766400,"objectID":"a52b01da614f8c3b3e5d8296ca122380","permalink":"/publication/22-ammf-tits/","publishdate":"2022-11-30T00:00:00Z","relpermalink":"/publication/22-ammf-tits/","section":"publication","summary":"Abstract: Recently significant progress has been made in 3D detection. However, it is still challenging to detect small contour objects under complex scenes. This paper proposes a novel Attention-based Multi-phase Multi-task Fusion (AMMF) that uses point-level, RoI-level, and multi-task fusions to complement the disadvantages of LiDAR and camera, to solve this challenge.","tags":["Robotics","Visual Computing","Deep Learning","Autonomous Driving"],"title":"AMMF: Attention-Based Multi-Phase Multi-Task Fusion for Small Contour Object 3D Detection","type":"publication"},{"authors":["Haitao Luo","Zongming Yang","Peng Yin","Johnell O. Brooks","Bing Li"],"categories":null,"content":"Abstract: Traditional manual wheelchairs have a fixed seat with no movement or angle adjustment, which can seriously affect the user\u0026rsquo;s comfort and greatly limit user experience. However, the electric wheelchair relies on strong intelligence and automatic features; it can not only realize the multidegree freedom adjustment of the human body and the seat but also has a rich and powerful man–machine control interface, which greatly facilitates and improves the user experience. This study upgraded a Permobil C400-powered wheelchair with multisensor data fusion technology to enrich its terrain recognition, tipping stability, and comfortability prediction. The tipping stability modeling of the wheelchair dummy system is carried out using multibody dynamics and vibration mechanics to obtain the tipping stability limit and the comfort evaluation of the wheelchair vibration acceleration on the human body during travel. Based on the elevation mapping method, the wheelchair can estimate the terrain from the local point of view at any point in time. At the same time, the RGB-D depth camera is connected to the robot operating system (ROS) system, and the open-source algorithm package RTAB-MAP is used to complete the MAP construction and collect the 3-D point-cloud terrain data. Then, the real 3-D terrain files are generated through the point-cloud stitching technology for stability simulation of the wheelchair–human system. The tipping stability and comfort indexes of the wheelchair–human system when passing over different physical terrains can be obtained. The experimental results show that the IMU data located on the human chest agree well with the simulation analysis data and are suitable for a variety of complex real-terrain conditions, verifying the accuracy of the wheelchair–human system dynamics model and the feasibility of the simulation analysis process. Thus, this modeling and simulation method can predict wheelchair stability and user comfortability well and ensure a high-performance experience.\n","date":1661212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661212800,"objectID":"048fd60d9a45f8374eefa36444844389","permalink":"/publication/22-wheelchair-thms/","publishdate":"2022-08-23T00:00:00Z","relpermalink":"/publication/22-wheelchair-thms/","section":"publication","summary":"Abstract: Traditional manual wheelchairs have a fixed seat with no movement or angle adjustment, which can seriously affect the user\u0026rsquo;s comfort and greatly limit user experience. However, the electric wheelchair relies on strong intelligence and automatic features; it can not only realize the multidegree freedom adjustment of the human body and the seat but also has a rich and powerful man–machine control interface, which greatly facilitates and improves the user experience.","tags":["Robotics","Human-Centered Computing"],"title":"Modeling and Prediction of User Stability and Comfortability on Autonomous Wheelchairs With 3D Mapping","type":"publication"},{"authors":["Johnell Brooks","Bing Li","Casey Jenkins","Loreta Dylgjeri","Sarath Krishna","Arjun K Ajayan","Madhuri Ghodekar","Sarvesh Nikhal","Akshay Anil Rana","ZongMing Yang"],"categories":null,"content":"Abstract: Individuals with visual impairments encounter many obstacles with passenger vehicles. This study aimed to increase the understanding of challenges specifically related to vehicles including ingress, in-vehicle considerations, comfort, and acceptance of ridesharing and transportation options for individuals who are visually impaired. Ten participants who are visually impaired, with an average age of 57.5 years, completed a semi-structured interview. The interview took place over Zoom or over the phone and focused on their passenger vehicle preferences and challenges, as well as what they would want for them to look like in the future. All of the participants typically requested rides from family and friends for local transportation, while only two used rideshare services. Half of the participants described the most common challenge when getting into a vehicle as hitting one’s head. All of the participants used their sense of touch to locate the seat belt, and most used touch and hearing to locate the vehicle and door they were getting into. When asked what they would like in the future for broader transportation needs, examples ranged from a talking cane, to an electronic guide dog, or to ear buds that could provide directions. Throughout the interviews, participants expressed the importance of transportation for them. This study increased the understanding of the challenges used when walking from an indoor environment to get to and into a vehicle. Understanding how individuals who are visually impaired currently get to and into a vehicle may aid engineers, new technology developers and O\u0026amp;M providers to create more processes and/or training that can help increase transportation options for those who are visually impaired.\n","date":1661126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661126400,"objectID":"f2420ff48f5d8cf706cba139ef047ee9","permalink":"/publication/22-seewayuser-bjvi/","publishdate":"2022-08-22T00:00:00Z","relpermalink":"/publication/22-seewayuser-bjvi/","section":"publication","summary":"Abstract: Individuals with visual impairments encounter many obstacles with passenger vehicles. This study aimed to increase the understanding of challenges specifically related to vehicles including ingress, in-vehicle considerations, comfort, and acceptance of ridesharing and transportation options for individuals who are visually impaired.","tags":null,"title":"Passenger Vehicle Preferences, Challenges, and Opportunities for Users Who Are Visually Impaired: An Exploratory Study","type":"publication"},{"authors":["Jesse Leaman","Zongming Yang","Yasmine Elglaly","Hung La","Bing Li"],"categories":null,"content":"Abstract: Assistive robots can be found in hospitals and rehabilitation clinics, where they help patients maintain a positive disposition. Our proposed robotic mobility solution combines state of the art hardware and software to provide a safer, more independent, and more productive lifestyle for people with some of the most severe disabilities. New hardware includes, a retractable roof, manipulator arm, a hard backpack, a number of sensors that collect environmental data and processors that generate 3D maps for a hands-free human-machine interface. The proposed new system receives input from the user via head tracking or voice command, and displays information through augmented reality into the user’s field of view. The software algorithm will use a novel cycle of self-learning artificial intelligence that achieves autonomous navigation while avoiding collisions with stationary and dynamic objects. The prototype will be assembled and tested over the next three years and a publicly available version could be ready two years thereafter.\n","date":1641168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641168000,"objectID":"9ffb43ad99032796315c90a401a9caed","permalink":"/publication/22-embodied-ai-smc/","publishdate":"2022-01-03T00:00:00Z","relpermalink":"/publication/22-embodied-ai-smc/","section":"publication","summary":"Abstract: Assistive robots can be found in hospitals and rehabilitation clinics, where they help patients maintain a positive disposition. Our proposed robotic mobility solution combines state of the art hardware and software to provide a safer, more independent, and more productive lifestyle for people with some of the most severe disabilities.","tags":null,"title":"Embodied-AI Wheelchair Framework with Hands-free Interface and Manipulation","type":"publication"},{"authors":["Zongming Yang","Liang Yang","Liren Kong","Ailin Wei","Jesse Leaman","Johnell Brooks","Bing Li"],"categories":null,"content":"Abstract: Assistive navigation for blind or visually impaired (BVI) individuals is of significance to extend their mobility and safety in traveling, enhancing their employment opportunities and fostering personal fulfillment. Conventional research is mainly based on robotic navigation approaches through localization, mapping, and path planning frameworks. They require heavy manual annotation of semantic information in maps and its alignment with sensor mapping. Inspired by the fact that we human beings naturally rely on language instruction inquiry and visual scene understanding to navigate in an unfamiliar environment, this paper proposes a novel vision-language model-based approach for BVI navigation. It does not need heavy-labeled indoor maps and provides a Safe and Efficient E-Wayfinding (SeeWay) assistive solution for BVI individuals. The system consists of a scene-graph map construction module, a navigation path generation module for global path inference by vision-language navigation (VLN), and a navigation with obstacle avoidance module for real-time local navigation. The SeeWay system was deployed on portable iPhone devices with cloud computing assistance for the VLN model inference. The field tests show the effectiveness of the VLN global path finding and local path re-planning. Experiments and quantitative results reveal that heuristic-style instruction outperforms direction/detailed-style instructions for VLN success rate (SR), and the SR decreases as the navigation length increases.\n","date":1641168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641168000,"objectID":"3397811d0231a56051c378d5d88eed1f","permalink":"/publication/22-seeway-smc/","publishdate":"2022-01-03T00:00:00Z","relpermalink":"/publication/22-seeway-smc/","section":"publication","summary":"Abstract: Assistive navigation for blind or visually impaired (BVI) individuals is of significance to extend their mobility and safety in traveling, enhancing their employment opportunities and fostering personal fulfillment. Conventional research is mainly based on robotic navigation approaches through localization, mapping, and path planning frameworks.","tags":null,"title":"SeeWay: Vision-Language Assistive Navigation for the Visually Impaired","type":"publication"},{"authors":["Satya R. Jaladi","Zhimin Chen","Narahari R. Malayanur","Raja M. Macherla","Bing Li"],"categories":null,"content":"Abstract: The current autonomous stack is well modularized and consists of perception, decision making and control in a handcrafted framework. With the advances in artificial intelligence (AI) and computing resources, researchers have been pushing the development of end-to-end AI for autonomous driving, at least in problems of small searching space such as in highway scenarios, and more and more photorealistic simulation will be critical for efficient learning. In this research, we propose a novel game-based end-to-end learning and testing framework for autonomous vehicle highway driving, by learning from human driving skills. Firstly, we utilize the popular game Grand Theft Auto V (GTA V) to collect highway driving data with our proposed programmable labels. Then, an end-to-end architecture predicts the steering and throttle values that control the vehicle by the image of the game screen. The predicted control values are sent to the game via a virtual controller to keep the vehicle in lane and avoid collisions with other vehicles on the road. The proposed solution is validated in GTA V games, and the results demonstrate the effectiveness of this end-to-end gamification framework for learning human driving skills.\n","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"31fe0c6b2b38f5997342742b97cf9b62","permalink":"/publication/22-end2end-driving-itsc/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/22-end2end-driving-itsc/","section":"publication","summary":"Abstract: The current autonomous stack is well modularized and consists of perception, decision making and control in a handcrafted framework. With the advances in artificial intelligence (AI) and computing resources, researchers have been pushing the development of end-to-end AI for autonomous driving, at least in problems of small searching space such as in highway scenarios, and more and more photorealistic simulation will be critical for efficient learning.","tags":null,"title":"End-To-End Training and Testing Gamification Framework to Learn Human Highway Driving","type":"publication"},{"authors":["Zhimin Chen","Longlong Jing","Liang Yang","Yingli Tian","Bing Li"],"categories":null,"content":"Abstract: In recent years, semi-supervised learning has been widely explored and shows excellent data efficiency for 2D data. There is an emerging need to improve data efficiency for 3D tasks due to the scarcity of labeled 3D data. This paper explores how the coherence of different modelities of 3D data (e.g. point cloud, image, and mesh) can be used to improve data efficiency for both 3D classification and retrieval tasks. We propose a novel multimodal semi-supervised learning framework by introducing instance-level consistency constraint and a novel multimodal contrastive prototype (M2CP) loss. The instance-level consistency enforces the network to generate consistent representations for multimodal data of the same object regardless of its modality. The M2CP maintains a multimodal prototype for each class and learns features with small intra-class variations by minimizing the feature distance of each object to its prototype while maximizing the distance to the others. Our proposed framework significantly outperforms all the state-of-the-art counterparts for both classification and retrieval tasks by a large margin on the modelNet10 and ModelNet40 datasets.\n","date":1632009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632009600,"objectID":"740a34e597e9b7f24e2f56fba6a59cd8","permalink":"/publication/21-m2cp-learning-bmvc/","publishdate":"2021-09-19T00:00:00Z","relpermalink":"/publication/21-m2cp-learning-bmvc/","section":"publication","summary":"Abstract: In recent years, semi-supervised learning has been widely explored and shows excellent data efficiency for 2D data. There is an emerging need to improve data efficiency for 3D tasks due to the scarcity of labeled 3D data.","tags":["Visual Computing","Deep Learning"],"title":"Multimodal Semi-Supervised Learning for 3D Objects","type":"publication"},{"authors":["Peng Yin","Lingyun Xu","Ziyue Feng","Anton Egorov","Bing Li"],"categories":null,"content":"Abstract: Accurate localization on the autonomous driving cars is essential for autonomy and driving safety, especially for complex urban streets and search-and-rescue subterranean environments where high-accurate GPS is not available. However current odometry estimation may introduce the drifting problems in long-term navigation without robust global localization. The main challenges involve scene divergence under the interference of dynamic environments and effective perception of observation and object layout variance from different viewpoints. To tackle these challenges, we present PSE-Match, a viewpoint-free place recognition method based on parallel semantic analysis of isolated semantic attributes from 3D point-cloud models. Compared with the original point cloud, the observed variance of semantic attributes is smaller. PSE-Match incorporates a divergence place learning network to capture different semantic attributes parallelly through the spherical harmonics domain. Using both existing benchmark datasets and two in-field collected datasets, our experiments show that the proposed method achieves above 70% average recall with top one retrieval and above 95% average recall with top ten retrieval cases. And PSE-Match has also demonstrated an obvious generalization ability with limited training dataset.\n","date":1629936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629936000,"objectID":"9fb6ae91d1dab2642f366931fd4ebe30","permalink":"/publication/21-place-recognition-tits/","publishdate":"2021-08-26T00:00:00Z","relpermalink":"/publication/21-place-recognition-tits/","section":"publication","summary":"Abstract: Accurate localization on the autonomous driving cars is essential for autonomy and driving safety, especially for complex urban streets and search-and-rescue subterranean environments where high-accurate GPS is not available. However current odometry estimation may introduce the drifting problems in long-term navigation without robust global localization.","tags":["Visual Computing","Deep Learning","Autonomous Driving"],"title":"PSE-Match: A Viewpoint-Free Place Recognition Method With Parallel Semantic Embedding","type":"publication"},{"authors":["Bangquan Xie","Zongming Yang","Liang Yang","Ruifa Luo","Ailin Wei","Xiaoxiong Weng","Bing Li"],"categories":null,"content":"Abstract: This paper proposes a real-time multi-scale semantic segmentation network (MsNet). MsNet is a combination of our novel multi-scale fusion with matching attention model (MFMA) as the decoding network and the network searched by asymptotic neural architecture search (ANAS) or MobileNetV3 as the encoding network. The MFMA not only extracts low level spatial features from multi-scale inputs but also decodes the contextual features extracted by ANAS. Specifically, considering the advantages and disadvantages of the addition fusion and concatenation fusion, we design multi-scale fusion (MF) that balances speed and accuracy. Then we creatively design two matching attention mechanisms (MA), including matching attention with low calculation (MALC) mechanism and matching attention with strong global context modeling (MASG) mechanism, to match varying resolutions and information of features at different levels of a network. Besides, the ANAS performs the deep neural network search by employing an asymptotic method and provide an efficient encoding network for MsNet, releasing researchers from those tedious mechanical trials. Through extensive experiments, we prove that MFMA, which can be applied to numerous recognition tasks, possesses excellent decoding ability. And we demonstrate the effectiveness and necessity of implementing the matching attention mechanism. Finally, the proposed two versions, MsNet-ANAS and MsNet-M achieve a new state-of-the-art trade-off between accuracy and speed on the CamVid and Cityscapes datasets. More remarkably, on the Nvidia Tesla V100 GPU, our MsNet-ANAS achieves 74.1% mIoU with the speed of 184.2 FPS on the CamVid while 72.9% mIoU with the speed of 119.9 FPS on the Cityscapes.\n","date":1629849600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629849600,"objectID":"97a21dbcaaaa0984d2854aec7bca1f30","permalink":"/publication/21-nas-attention-tits/","publishdate":"2021-08-25T00:00:00Z","relpermalink":"/publication/21-nas-attention-tits/","section":"publication","summary":"Abstract: This paper proposes a real-time multi-scale semantic segmentation network (MsNet). MsNet is a combination of our novel multi-scale fusion with matching attention model (MFMA) as the decoding network and the network searched by asymptotic neural architecture search (ANAS) or MobileNetV3 as the encoding network.","tags":["Robotics","Visual Computing","Deep Learning","Autonomous Driving"],"title":"Multi-Scale Fusion With Matching Attention Model: A Novel Decoding Network Cooperated With NAS for Real-Time Semantic Segmentation","type":"publication"},{"authors":["Howard J.J. Brand","Bing Li"],"categories":null,"content":"Abstract: Military ground vehicles operate in off-road environments traversing different terrains under various environmental conditions. There has been an increasing interest towards autonomous off-road vehicle navigation, leading to the needs of terrain traversability assessment through sensing. These methods utilized data-driven approaches on classical robotic perception sensing modalities (RGB cameras, Lidar, and depth cameras) positioned in front of ground vehicles in order to observe approaching terrain. Classical robotic sensing modalities, though effective for describing environment geometry and object detection and tracking, aren’t able to directly observe features related to compaction and moisture content which have significant effects on the moduli properties governing terrain mechanics. These methods then become very specialized to specific regions and environmental conditions which are inevitably subject to change. Radio wave-based sensing modes have been shown in studies to have success in observing different terrain surface and subsurface conditions such as compaction and moisture presence. We study the usability of emerging, portable and front mountable radar imaging sensors to provide real-time radio spectra information of the in-coming terrain area. In this study, we use a radar transceiver array operating in the 6.2-6.9 GHz spectral range to develop a radar image/soil moisture dataset, where beamforming is used to recover radar images of lab soil samples of various moisture content levels. The radar images are constructed at various distances from the soil surface and various spatial resolutions to support a local path planning scenario. Support vector machine (SVM) classifier and support vector regression (SVR) models are trained on the dataset and tested on lab data and in-field data. Classifier and regression model results indicate that normalized local radar image statistics are able to distinguish moisture levels at various distances and spatial resolutions.\n","date":1617667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617667200,"objectID":"bffe9d8e4ad4337d2ccafafe0d086318","permalink":"/publication/21-mmwave-radar-sae/","publishdate":"2021-04-06T00:00:00Z","relpermalink":"/publication/21-mmwave-radar-sae/","section":"publication","summary":"Abstract: Military ground vehicles operate in off-road environments traversing different terrains under various environmental conditions. There has been an increasing interest towards autonomous off-road vehicle navigation, leading to the needs of terrain traversability assessment through sensing.","tags":["Sensing","Autonomous Driving"],"title":"Nondestructive Evaluation of Terrain Using mmWave Radar Imaging","type":"publication"},{"authors":["Aryan Naveen","Haitao Luo","Zhimin Chen","Bing Li"],"categories":null,"content":"Abstract: Autonomous wheelchairs can address a very large need in many populations by serving as the gateway to a much higher degree of independence and mobility capability. This is due to the fact that the big picture idea for autonomous wheelchairs integration into the transportation chain is to allow for individuals to be able to utilize the Intelligent wheelchair to reach the vehicle (regardless of terrain), mount into autonomous wheelchair that navigates to desired destination, and finally autonomous wheelchair dismounts. This will enable a higher degree of mobility for a handicapped population that experiences a large quantity of restrictions as a result of their circumstances. In order for this potential to be achieved numerous precautions must be integrated into the control system, such as stability maintenance. This paper focuses on mapping the environment through the use of a LiDAR sensor and predicting the stability of the given wheelchair. We utilize RTAB Mapping in combination with LiDAR odometry to construct a 3D map of the environment. Then Poisson reconstruction is deployed to convert the built 3D pointcloud into triangular mesh that allows for the norms to the surface to be calculated, which allows for stability prediction. This paper, not only outlines a novel pipeline but also deployed the pipeline on the recently released Intel RealSense L515 sensor and leverages its unique capabilities.\n","date":1605830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605830400,"objectID":"b84198e88d0e1edbab301ac93e06f8d2","permalink":"/publication/20-wheelchair-stability-cyber/","publishdate":"2020-11-20T00:00:00Z","relpermalink":"/publication/20-wheelchair-stability-cyber/","section":"publication","summary":"Abstract: Autonomous wheelchairs can address a very large need in many populations by serving as the gateway to a much higher degree of independence and mobility capability. This is due to the fact that the big picture idea for autonomous wheelchairs integration into the transportation chain is to allow for individuals to be able to utilize the Intelligent wheelchair to reach the vehicle (regardless of terrain), mount into autonomous wheelchair that navigates to desired destination, and finally autonomous wheelchair dismounts.","tags":["Robotics","Visual Computing","Sensing"],"title":"3D Mapping and Stability Prediction for Autonomous Wheelchairs","type":"publication"},{"authors":["Howard J.J. Brand","Bing Li"],"categories":null,"content":"Abstract: The fundamental aspect of unmanned ground vehicle (UGV) navigation, especially over off-road environments, are representations of terrain describing geometry, types, and traversability. One of the typical representations of the environment is digital surface models (DSMs) which efficiently encode geometric information. In this research, we propose a collaborative approach for UGV navigation through unmanned aerial vehicle (UAV) mapping to create semantic DSMs, by leveraging the UAV wide field of view and nadir perspective for map surveying. Semantic segmentation models for terrain recognition are affected by sensing modality as well as dataset availability. We explored and developed semantic segmentation deep convolutional neural networks (CNN) models to construct semantic DSMs. We further conducted a thorough quantitative and qualitative analysis regarding image modalities (between RGB, RGB+DSM and RG+DSM) and dataset availability effects on the performance of segmentation CNN models.\n","date":1597276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597276800,"objectID":"7a9a98a02c25dd330eaf1145e024e314","permalink":"/publication/20-gvsets-mapping/","publishdate":"2020-08-13T00:00:00Z","relpermalink":"/publication/20-gvsets-mapping/","section":"publication","summary":"Abstract: The fundamental aspect of unmanned ground vehicle (UGV) navigation, especially over off-road environments, are representations of terrain describing geometry, types, and traversability. One of the typical representations of the environment is digital surface models (DSMs) which efficiently encode geometric information.","tags":["Robotics","Visual Computing","Sensing","Deep Learning","Autonomous Driving"],"title":"Semantic Digital Surface Map Towards Collaborative Off-Road Vehicle Autonomy","type":"publication"},{"authors":["Haitao Luo","Zhimin Chen","Aryan Naveen","Bing Li"],"categories":null,"content":"Abstract: With the particular passage capability, all-terrain vehicle (ATV) has been widely used for off-road scenarios. In this research, we conduct a lateral sway stability analysis for the suspension mechanism of a general vehicle and establish a mathematical model of static and dynamic stability based on the maximum lateral sway angle and lateral sway acceleration, by considering the combined angular stiffness of independent suspension, angular stiffness of the lateral stabilizer bar and vertical stiffness of tires. 3D point cloud data of a terrain environment is collected using an RGB-Depth camera, and a triangular topography map is constructed. The results in ADAMS show that the proposed stability model can accurately predict the critical tipping state of the vehicle, and the method deployed for real-world terrain modeling and simulation analysis is generalizable for the stability assessment of the interaction between ATV and real-world terrain.\n","date":1597104000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597104000,"objectID":"65945edddfa87769dec06a3f84a14b82","permalink":"/publication/20-gvsets-atv-stability/","publishdate":"2020-08-11T00:00:00Z","relpermalink":"/publication/20-gvsets-atv-stability/","section":"publication","summary":"Abstract: With the particular passage capability, all-terrain vehicle (ATV) has been widely used for off-road scenarios. In this research, we conduct a lateral sway stability analysis for the suspension mechanism of a general vehicle and establish a mathematical model of static and dynamic stability based on the maximum lateral sway angle and lateral sway acceleration, by considering the combined angular stiffness of independent suspension, angular stiffness of the lateral stabilizer bar and vertical stiffness of tires.","tags":["Robotics","Autonomous Driving"],"title":"Dynamic Modeling and Prediction of Rollover Stability for All-Terrain Vehicles","type":"publication"},{"authors":["Liang Yang","Bing Li","Wei Li","Howard J.J. Brand","Biao Jiang","Jizhong Xiao"],"categories":null,"content":"Abstract: The concrete aging problem has gained more attention in recent years as more bridges and tunnels in the United States lack proper maintenance. Though the Federal Highway Administration requires these public concrete structures to be inspected regularly, on-site manual inspection by human operators is time-consuming and labor-intensive. Conventional inspection approaches for concrete inspection, using RGB imagebased thresholding methods, are not able to determine metric information as well as accurate location information for assessed defects for conditions. To address this challenge, we propose a deep neural network (DNN) based concrete inspection system using a quadrotor flying robot (referred to as CityFlyer) mounted with an RGB-D camera. The inspection system introduces several novel modules. Firstly, a visual-inertial fusion approach is introduced to perform camera and robot positioning and structure 3D metric reconstruction. The reconstructed map is used to retrieve the location and metric information of the defects. Secondly, we introduce a DNN model, namely AdaNet, to detect concrete spalling and cracking, with the capability of maintaining robustness under various distances between the camera and concrete surface. In order to train the model, we craft a new dataset, i.e., the concrete structure spalling and cracking (CSSC) dataset, which is released publicly to the research community. Finally, we introduce a 3D semantic mapping method using the annotated framework to reconstruct the concrete structure for visualization. We performed comparative studies and demonstrated that our AdaNet can achieve 8.41% higher detection accuracy than ResNets and VGGs. Moreover, we conducted five field tests, of which three are manual hand-held tests and two are drone-based field tests. These results indicate that our system is capable of performing metric field inspection, and can serve as an effective tool for civil engineers.\n","date":1593388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593388800,"objectID":"db2d00a0497e346a34eb2fdf473447ad","permalink":"/publication/20-inspection-drone/","publishdate":"2020-06-29T00:00:00Z","relpermalink":"/publication/20-inspection-drone/","section":"publication","summary":"Abstract: The concrete aging problem has gained more attention in recent years as more bridges and tunnels in the United States lack proper maintenance. Though the Federal Highway Administration requires these public concrete structures to be inspected regularly, on-site manual inspection by human operators is time-consuming and labor-intensive.","tags":["Robotics","Visual Computing","Sensing","Deep Learning"],"title":"Concrete Defects Inspection and 3D Mapping Using CityFlyer Quadrotor Robot","type":"publication"},{"authors":["Aryan Naveen","Haitao Luo","Zhimin Chen","Bing Li"],"categories":null,"content":"Abstract: Handicapped individuals often rely heavily on various assistive technologies including wheelchairs and the purpose of these technologies is to enable greater levels of independence for the user. In the development of autonomous wheelchairs, it is imperative that the wheelchair maintains appropriate stability for the user in an outdoor urban environment. This paper proposes an RGB-Depth based perception algorithm for 3D mapping of the environment in addition to dynamic modeling of the wheelchair for stability analysis and prediction. We utilize RTAB Mapping in combination with Poisson Reconstruction that produced triangular mesh from which an accurate prediction of the stability of the wheelchair can be made based on the normals and the critical angle calculated from the dynamic model of the wheelchair.\n","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"4297ef4884fb40c86c0cffa29efacc44","permalink":"/publication/20-wheelchair-curb-icchp/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/20-wheelchair-curb-icchp/","section":"publication","summary":"Abstract: Handicapped individuals often rely heavily on various assistive technologies including wheelchairs and the purpose of these technologies is to enable greater levels of independence for the user. In the development of autonomous wheelchairs, it is imperative that the wheelchair maintains appropriate stability for the user in an outdoor urban environment.","tags":["Robotics","Visual Computing","Sensing","Human-Centered Computing"],"title":"Predicting Wheelchair Stability While Crossing a Curb Using RGB-Depth Vision","type":"publication"},{"authors":["Rui Li","Howard J.J. Brand","Aditya Gopinath","Srivatsav Kamarajugadda","Liang Yang","Weitian Wang","Bing Li"],"categories":null,"content":"Abstract: Driving inattention caused by drowsiness has been a significant reason for vehicle crash accidents, and there is a critical need to augment driving safety by monitoring driver drowsiness behaviors. For real-time drowsy driving awareness, we propose a vision-based driver drowsiness monitoring system (DDMS) for driver drowsiness behavior recognition and analysis. First, an infrared camera is deployed in-vehicle to capture the driver’s facial and head information in naturalistic driving scenarios, in which the driver may or may not wear glasses or sunglasses. Second, we propose and design a multi-modal features representation approach based on facial landmarks, and head pose which is retrieved in a convolutional neural network (CNN) regression model. Finally, an extreme learning machine (ELM) model is proposed to fuse the facial landmark, recognition model and pose orientation for drowsiness detection. The DDMS gives promptly warning to the driver once a drowsiness event is detected. The proposed CNN and ELM models are trained in a drowsy driving dataset and are validated on public datasets and field tests. Comparing to the end-to-end CNN recognition model, the proposed multi-modal fusion with the ELM detection model allows faster and more accurate detection with minimal intervention. The experimental result demonstrates that DDMS is able to provide real-time and effective drowsy driving alerts under various light conditions to augment driving safety.\n","date":1586822400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586822400,"objectID":"b1c49a13152fb3ad996751e7a2b516b0","permalink":"/publication/20-driver-drowsiness-sae/","publishdate":"2020-04-14T00:00:00Z","relpermalink":"/publication/20-driver-drowsiness-sae/","section":"publication","summary":"Abstract: Driving inattention caused by drowsiness has been a significant reason for vehicle crash accidents, and there is a critical need to augment driving safety by monitoring driver drowsiness behaviors. For real-time drowsy driving awareness, we propose a vision-based driver drowsiness monitoring system (DDMS) for driver drowsiness behavior recognition and analysis.","tags":["Visual Computing","Sensing","Deep Learning","Human-Centered Computing","Autonomous Driving"],"title":"Driver Drowsiness Behavior Detection and Analysis Using Vision-Based Multimodal Features for Driving Safety","type":"publication"},{"authors":["Bing Li","Rich Valde"],"categories":null,"content":"Abstract: Embodiments described herein may provide a method for generating a three-dimensional vector model of the interior of a structure. Methods may include: receiving sensor data indicative of a trajectory; receiving sensor data defining structural surfaces within a structure; generating a three-dimensional point cloud from the sensor data defining structural surfaces within the structure; segmenting the three-dimensional point cloud into two or more segments based, at least in part, on the sensor data indicative of trajectory; generating a three-dimensional surface model of an interior of the structure based on the segmented three-dimensional point cloud with semantic recognition and labelling; and providing the three-dimensional surface model of an interior of the structure to an advanced driver assistance system to facilitate autonomous vehicle parking.\n","date":1579564800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579564800,"objectID":"4adbbaea26df581a90410b6260ff2715","permalink":"/publication/20-mapping-patent/","publishdate":"2020-01-21T00:00:00Z","relpermalink":"/publication/20-mapping-patent/","section":"publication","summary":"Abstract: Embodiments described herein may provide a method for generating a three-dimensional vector model of the interior of a structure. Methods may include: receiving sensor data indicative of a trajectory; receiving sensor data defining structural surfaces within a structure; generating a three-dimensional point cloud from the sensor data defining structural surfaces within the structure; segmenting the three-dimensional point cloud into two or more segments based, at least in part, on the sensor data indicative of trajectory; generating a three-dimensional surface model of an interior of the structure based on the segmented three-dimensional point cloud with semantic recognition and labelling; and providing the three-dimensional surface model of an interior of the structure to an advanced driver assistance system to facilitate autonomous vehicle parking.","tags":["Robotics","Visual Computing","Sensing","Autonomous Driving"],"title":"Method, Apparatus and Computer Program Product for Mapping and Modeling a Three Dimensional Structure","type":"publication"},{"authors":["Liang Yang","Bing Li","Guoyong Yang","Yong Chang","Zhaoming Liu","Biao Jiang","Jizhong Xiao"],"categories":null,"content":"Abstract: This paper presents a novel metric inspection robot system using a deep neural network to detect and measure surface flaws (i.e., crack and spalling) on concrete structures performed by a wall-climbing robot. The system consists of four modules: robotics data collection module to obtain RGB-D images and IMU measurement, visual-inertial SLAM module to generate pose coupled key-frames with depth information, InspectionNet module to classify each pixel into three classes (back-ground, crack and spalling), and 3D registration and map fusion module to register the flaw patch into registered 3D model overlaid and highlighted with detected flaws for spatial-contextual visualization. The system enables the metric model of each surface flaw patch with pixel-level accuracy and determines its location in 3D space that is significant for structural health assessment and monitoring. The InspectionNet achieves an average accuracy of 87.64% for crack and spalling inspection. We also demonstrate our InspectionNet is robust to view angle, scale and illumination variation. Finally, we design a metric voxel volume map to highlight the flaw in 3D model and provide location and metric information.\n","date":1572739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572739200,"objectID":"39d5e47bce884be9b2b5945ccf5a0380","permalink":"/publication/19-visual-inspection-iros/","publishdate":"2019-11-03T00:00:00Z","relpermalink":"/publication/19-visual-inspection-iros/","section":"publication","summary":"Abstract: This paper presents a novel metric inspection robot system using a deep neural network to detect and measure surface flaws (i.e., crack and spalling) on concrete structures performed by a wall-climbing robot.","tags":["Robotics","Visual Computing","Sensing","Deep Learning"],"title":"Deep Neural Network based Visual Inspection with 3D Metric Measurement of Concrete Defects using Wall-climbing Robot","type":"publication"},{"authors":["Bing Li","Juan Pablo Munoz","Xuejian Rong","Qingtian Chen","Jizhong Xiao","Yingli Tian","Aries Arditi","Mohammed Yousuf"],"categories":null,"content":"Abstract: This paper presents a new holistic vision-based mobile assistive navigation system to help blind and visually impaired people with indoor independent travel. The system detects dynamic obstacles and adjusts path planning in real-time to improve navigation safety. First, we develop an indoor map editor to parse geometric information from architectural models and generate a semantic map consisting of a global 2D traversable grid map layer and context-aware layers. By leveraging the visual positioning service (VPS) within the Google Tango device, we design a map alignment algorithm to bridge the visual area description file (ADF) and semantic map to achieve semantic localization. Using the on-board RGB-D camera, we develop an efficient obstacle detection and avoidance approach based on a time-stamped map Kalman filter (TSM-KF) algorithm. A multi-modal human-machine interface (HMI) is designed with speech-audio interaction and robust haptic interaction through an electronic SmartCane. Finally, field experiments by blindfolded and blind subjects demonstrate that the proposed system provides an effective tool to help blind individuals with indoor navigation and wayfinding.\n","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"b62f17648e669c50615d1a330a3cc7e3","permalink":"/publication/19-visual-mobile-navi-tmc/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/19-visual-mobile-navi-tmc/","section":"publication","summary":"Abstract: This paper presents a new holistic vision-based mobile assistive navigation system to help blind and visually impaired people with indoor independent travel. The system detects dynamic obstacles and adjusts path planning in real-time to improve navigation safety.","tags":["Robotics","Visual Computing","Sensing","Human-Centered Computing"],"title":"Vision-Based Mobile Indoor Assistive Navigation Aid for Blind People","type":"publication"},{"authors":["Xiaoming Zhao","Weihai Chen","Bing Li","Xingming Wu","Jianhua Wang"],"categories":null,"content":"Abstract: The mobility on stairways is a daily challenge for seniors and people with dyskinesia. Lower limb exoskeletons can be effective assistants to improve their life quality. In this paper, we present an adaptive stair-ascending gait generation algorithm based on a depth camera for lower limb exoskeletons. We first construct a linked-list-based stairway model with the point cloud captured from the depth camera. Then, an optimal foothold point is calculated based on the linked-list stair model for gait generation. Finally, the exoskeleton takes the stair-ascending gait of healthy people as a reference and generates appropriate gait for the stair. The proposed gait generation algorithm is initially validated through holistic simulation analyses. We tested the stairway modeling algorithm on varieties of indoor and outdoor stairways and evaluated the gait generation algorithm on stairs of different height. The subjects’ stair walking tests with lower limb exoskeletons show the effectiveness of the proposed stairway modeling and gait generation approaches.\n","date":1547596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547596800,"objectID":"a714ea188253e588e80f526053ed50ad","permalink":"/publication/19-depth-gait/","publishdate":"2019-01-16T00:00:00Z","relpermalink":"/publication/19-depth-gait/","section":"publication","summary":"Abstract: The mobility on stairways is a daily challenge for seniors and people with dyskinesia. Lower limb exoskeletons can be effective assistants to improve their life quality. In this paper, we present an adaptive stair-ascending gait generation algorithm based on a depth camera for lower limb exoskeletons.","tags":["Robotics","Sensing","Human-Centered Computing"],"title":"An Adaptive Stair-ascending Gait Generation Approach Based on Depth Camera for Lower Limb Exoskeleton.” Review of Scientific Instruments","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"1e799e866620c90587eaa350f7cc0556","permalink":"/all_publications/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/all_publications/","section":"","summary":"Hello!","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"52dc1e63f96b46ea730c275a0d44b70f","permalink":"/openings/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/openings/","section":"","summary":"Hello!","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"/people/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"Hello!","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"f1d044c0738ab9f19347f15c290a71a1","permalink":"/research/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/research/","section":"","summary":"Hello!","tags":null,"title":"","type":"widget_page"},{"authors":["Liang Yang","Bing Li","Wei Li","Biao Jiang","Jizhong Xiao"],"categories":null,"content":"Abstract: In this paper, we exploit the concrete surface flaw inspection through the fusion of visual positioning and semantic segmentation approach. The fused inspection result is represented by a 3D metric map with a spatial area, width, and depth information, which shows the advantage over general inspection in image space without metric info. We also relieve the human labor with an automatic labeling approach. The system is composed of three hybrid parts: visual positioning to enable pose association, crack/spalling inspection using a deep neural network (pixel level), and a 3D random field filter for fusion to achieve a global 3D metric map. To improve the infrastructure inspection, we released a new data set for concrete crack and spalling segmentation which is built on CSSC dataset [27]. To leverage the effectiveness of the large-scale SLAM aided semantic inspection, we performed three field tests and one baseline test. Experimental results show that our proposed approach significantly improves the capability of 3D metric concrete inspection via deploying visual SLAM. Furthermore, we achieve an 82.4% MaxF1 score for crack detection and 88.64% MaxF1 score for spalling detection on the relabeled dataset.\n","date":1529280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529280000,"objectID":"805e0e559f26bf099f4e638768cb5b4a","permalink":"/publication/18-metric-recon-cvprw/","publishdate":"2018-06-18T00:00:00Z","relpermalink":"/publication/18-metric-recon-cvprw/","section":"publication","summary":"Abstract: In this paper, we exploit the concrete surface flaw inspection through the fusion of visual positioning and semantic segmentation approach. The fused inspection result is represented by a 3D metric map with a spatial area, width, and depth information, which shows the advantage over general inspection in image space without metric info.","tags":["Robotics","Visual Computing","Sensing","Deep Learning"],"title":"Semantic Metric 3D Reconstruction for Concrete Inspection","type":"publication"},{"authors":["Bing Li","Liang Yang","Jizhong Xiao","Rich Valde","Michael Wrenn","Jim Leflar"],"categories":null,"content":"Abstract: We present a novel collaborative mapping and autonomous parking system for semi-structured multi-story parking garages, based on cooperative 3-D LiDAR point cloud registration and Bayesian probabilistic updating. First, an inertial-enhanced (IE) generalized iterative closest point (G-ICP) approach is presented to perform high accuracy registration for LiDAR odometry, which is loosely coupled with inertial measurement unit using multi-state extended Kalman filter fusion. Second, the IE G-ICP is utilized to reconstruct the 3-D point cloud model for each vehicle, and then the individual model maps are merged and updated into a global probabilistic 2-D grid map. A collaborative multiple layer semantic map is constructed to support autonomous parking. Finally, we propose a collaborative navigation approach for path planning when there are multiple vehicles in the parking garage through vehicle-to-vehicle communication. A global path planner is designed to explore the minimum cost path based on the semantic map, and local motion planning is performed using a random exploring algorithm for obstacle avoidance and path smoothing. Our pilot experimental evaluation provides a proof of concept for indoor autonomous parking by collaborative perception, map merging, and updating methodologies.\n","date":1520380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520380800,"objectID":"6fb89460e7d6bd5554e6dede56ebecdc","permalink":"/publication/18-colla-mapping-tits/","publishdate":"2018-03-07T00:00:00Z","relpermalink":"/publication/18-colla-mapping-tits/","section":"publication","summary":"Abstract: We present a novel collaborative mapping and autonomous parking system for semi-structured multi-story parking garages, based on cooperative 3-D LiDAR point cloud registration and Bayesian probabilistic updating. First, an inertial-enhanced (IE) generalized iterative closest point (G-ICP) approach is presented to perform high accuracy registration for LiDAR odometry, which is loosely coupled with inertial measurement unit using multi-state extended Kalman filter fusion.","tags":["Robotics","Visual Computing","Sensing","Deep Learning","Autonomous Driving"],"title":"Collaborative Mapping and Autonomous Parking for Multi-Story Parking Garage","type":"publication"},{"authors":["Liang Yang","Yuqing He","Jizhong Xiao","Bing Li","Zhaoming Liu"],"categories":null,"content":"Abstract: This paper presents a novel randomized path planning algorithm, which is a goal and homology biased sampling based algorithm called Multiple Guiding Attraction based Random Tree, and robots can use it to tackle pop-up and moving threats under kinodynamic constraints. Our proposed method considers the kinematics and dynamics constraints, using obstacle information to perform informed sampling and redistribution around collision region toward valid routing. We pioneeringly propose a multiple path planning method using ‘Extending Forbidden’ algorithm, rather than using variant cost principles for online threat management. The threat management method performs online path switching between the planned multiple paths, which is proved with better time performance than conventional approaches. The proposed method has advantage in exploration in obstacle crowded environment, where narrow corridor fails using the general sampling based exploration methods. We perform detailed comparative experiments with peer approaches in cluttered environment, and point out the advantages in time and mission performance.\n","date":1513728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513728000,"objectID":"61222ae6b7f0ed1abd3cd7fb65188c5a","permalink":"/publication/17-multi-traj-path-plan/","publishdate":"2017-12-20T00:00:00Z","relpermalink":"/publication/17-multi-traj-path-plan/","section":"publication","summary":"Abstract: This paper presents a novel randomized path planning algorithm, which is a goal and homology biased sampling based algorithm called Multiple Guiding Attraction based Random Tree, and robots can use it to tackle pop-up and moving threats under kinodynamic constraints.","tags":["Robotics","Autonomous Driving"],"title":"A Random Multi-trajectory Generation Method for Online Emergency Threat Management (Analysis and Application in Path Planning Algorithm)","type":"publication"},{"authors":["Liang Yang","Bing Li","Wei Li","Zhaoming Liu","Guoyong Yang","Jizhong Xiao"],"categories":null,"content":"Abstract: Concrete spalling and crack inspection is a labor intensive and routine task. However, it plays an important role in structure health monitoring (SHM) of civil infrastructures. Autonomous inspection with robots has been regarded as one of the best ways to reduce both error and cost. This paper presents an automated approach using Unmanned Aerial Vehicle(UAV) and towards a Concrete Structure Spalling and Crack database (CSSC), which is by far the first released database for deep learning inspection. We aim locate the spalling and crack regions to assist 3D registration and visualization. For deep inspection, we provide a complete procedure of data searching, labeling, training, and post processing. We further present a visual Simultaneously Localization and Mapping(SLAM) approach for localization and reconstruction. Comparative experiments and field tests are illustrated, results show that we can achieve an accuracy over 70% for field tests, and more than 93% accuracy with CSSC database.\n","date":1512432000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512432000,"objectID":"f1d9afdc41a5b0bb0d53c9bd6f6c8b8d","permalink":"/publication/17-spalling-crack-db-robio/","publishdate":"2017-12-05T00:00:00Z","relpermalink":"/publication/17-spalling-crack-db-robio/","section":"publication","summary":"Abstract: Concrete spalling and crack inspection is a labor intensive and routine task. However, it plays an important role in structure health monitoring (SHM) of civil infrastructures. Autonomous inspection with robots has been regarded as one of the best ways to reduce both error and cost.","tags":["Visual Computing","Sensing","Deep Learning"],"title":"A Robotic System Towards Concrete Structure Spalling and Crack Database","type":"publication"},{"authors":["Juan Pablo Munoz","Bing Li","Xuejian Rong","Jizhong Xiao","Yingli Tian","Aries Arditi"],"categories":null,"content":"Abstract: This paper presents an innovative wearable system to assist visually impaired people navigate indoors in real time. Our proposed system incorporates state-of-the-art handheld devices from Google\u0026rsquo;s Project Tango and integrates path planner and obstacle avoidance submodules, as well as human-computer interaction techniques, to provide assistance to the user. Our system preprocesses a priori knowledge of the environment extracted from CAD files and spatial information from Google\u0026rsquo;s Area Description Files. The system can then reallocate resources to navigation and human-computer interaction tasks during execution. The system is capable of exploring complex environments spanning multiple floors and has been tested and demonstrated in a variety of indoor environments.\n","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"53dda4e29e00d7000e0708131187f946","permalink":"/publication/17-indoor-navi-cyber/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/publication/17-indoor-navi-cyber/","section":"publication","summary":"Abstract: This paper presents an innovative wearable system to assist visually impaired people navigate indoors in real time. Our proposed system incorporates state-of-the-art handheld devices from Google\u0026rsquo;s Project Tango and integrates path planner and obstacle avoidance submodules, as well as human-computer interaction techniques, to provide assistance to the user.","tags":["Robotics","Visual Computing","Human-Centered Computing"],"title":"An Assistive Indoor Navigation System for the Visually Impaired in Multi-Floor Environments (Best Conference Paper Award)","type":"publication"},{"authors":["Qingtian Chen","Muhammad Khan","Christina Tsangouri","Christopher Yang","Bing Li","Jizhong Xiao","Zhigang Zhu"],"categories":null,"content":"Abstract: This paper presents SmartCane - the CCNY Smart Cane system, a robotic white cane and mobile device navigation software for visually impaired people. The system includes software for Google Tango devices that utilizes simultaneous localization and mapping (SLAM) to plan a path and guide a visually impaired user to waypoints within indoor environments. A control panel is mounted on the standard white cane that enables visually impaired users to communicate with the navigation software and is additionally used to provide navigation instructions via haptic feedback. Based on the motion-tracking and localization capabilities of the Google Tango, the SmartCane is able to generate a safe path to the destination waypoint indicated by the user.\n","date":1501459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501459200,"objectID":"891845ded3000d2e8c90625773e02341","permalink":"/publication/17-smart-cane-cyber/","publishdate":"2017-07-31T00:00:00Z","relpermalink":"/publication/17-smart-cane-cyber/","section":"publication","summary":"Abstract: This paper presents SmartCane - the CCNY Smart Cane system, a robotic white cane and mobile device navigation software for visually impaired people. The system includes software for Google Tango devices that utilizes simultaneous localization and mapping (SLAM) to plan a path and guide a visually impaired user to waypoints within indoor environments.","tags":["Human-Centered Computing"],"title":"CCNY Smart Cane","type":"publication"},{"authors":["Jizhong Xiao","Bing Li","Kenshin Ushiroda","Qiang Song"],"categories":null,"content":"Abstract: This paper presents Rise-Rover, a new generation wall-climbing robot with high reliability and load-carrying capacity on vertical surfaces for Non-Destructive Testing (NDT) of concrete and steel infrastructure. One Rise-Rover drivetrain module can operate on both smooth and rough vertical/inclined surfaces independently. The on-board electronics and PID controller monitor the pressure reading and adjust impeller speed to provide stable suction force for the wall-climbing robot. The use of duct fan and tether further increase the operation reliability. Rise-Rover is remotely controlled by Android smartphone via Wifi, and the User Interface (UI) provides good usability and convenience. The experimental test verified the good performance of the Rise-Rover prototype.\n","date":1501459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501459200,"objectID":"a66e660a732784a8c30e28e369a9a7f0","permalink":"/publication/15-wall-climbing-clawar/","publishdate":"2017-07-31T00:00:00Z","relpermalink":"/publication/15-wall-climbing-clawar/","section":"publication","summary":"Abstract: This paper presents Rise-Rover, a new generation wall-climbing robot with high reliability and load-carrying capacity on vertical surfaces for Non-Destructive Testing (NDT) of concrete and steel infrastructure. One Rise-Rover drivetrain module can operate on both smooth and rough vertical/inclined surfaces independently.","tags":["Robotics"],"title":"Rise-Rover: A Wall-Climbing Robot with High Reliability and Load-Carrying Capacity (Industrial Robot Innovation Award)","type":"publication"},{"authors":["Bing Li","Kenshin Ushiroda","Liang Yang","Qiang Song","Jizhong Xiao"],"categories":null,"content":"Abstract: The impact-echo (IE) acoustic inspection method is a non-destructive evaluation technique, which has been widely applied to detect the defects, structural deterioration level, and thickness of plate-like concrete structures. This paper presents a novel climbing robot, namely Rise-Rover, to perform automated IE signal collection from concrete structures with IE signal analyzing based on machine learning techniques. Rise-Rover is our new generation robot, and it has a novel and enhanced absorption system to support heavy load, and crawler-like suction cups to maintain high mobility performance while crossing small grooves. Moreover, the design enables a seamless transition between ground and wall. This paper applies the fast Fourier transform and wavelet transform for feature detection from collected IE signals. A distance metric learning based support vector machine approach is newly proposed to automatically classify the IE signals. With the visual-inertial odometry of the robot, the detected flaws of inspection area on the concrete plates are visualized in 2D/3D. Field tests on a concrete bridge deck demonstrate the efficiency of the proposed robot system in automatic health condition assessment for concrete structures.\n","date":1501459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501459200,"objectID":"50cebfd99280e55ff42c2abd5753ed01","permalink":"/publication/17-wall-climbing-inspection/","publishdate":"2017-07-31T00:00:00Z","relpermalink":"/publication/17-wall-climbing-inspection/","section":"publication","summary":"Abstract: The impact-echo (IE) acoustic inspection method is a non-destructive evaluation technique, which has been widely applied to detect the defects, structural deterioration level, and thickness of plate-like concrete structures. This paper presents a novel climbing robot, namely Rise-Rover, to perform automated IE signal collection from concrete structures with IE signal analyzing based on machine learning techniques.","tags":["Robotics","Sensing"],"title":"Wall-climbing Robot for Non-destructive Evaluation using Impact-echo and Metric Learning SVM","type":"publication"},{"authors":["Xuejian Rong","Bing Li","Juan Pablo Munoz","Jizhong Xiao","Aries Arditi","Yingli Tian"],"categories":null,"content":"Abstract: Scene text in indoor environments usually preserves and communicates important contextual information which can significantly enhance the independent travel of blind and visually impaired people. In this paper, we present an assistive text spotting navigation system based on an RGB-D mobile device for blind or severely visually impaired people. Specifically, a novel spatial-temporal text localization algorithm is proposed to localize and prune text regions, by integrating stroke-specific features with a subsequent text tracking process. The density of extracted text-specific feature points serves as an efficient text indicator to guide the user closer to text-likely regions for better recognition performance. Next, detected text regions are binarized and recognized by off-the-shelf optical character recognition methods. Significant non-text indicator signage can also be matched to provide additional environment information. Both recognized results are then transferred to speech feedback for user interaction. Our proposed video text localization approach is quantitatively evaluated on the ICDAR 2013 dataset, and the experimental results demonstrate the effectiveness of our proposed method.\n","date":1481328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481328000,"objectID":"bbb2c014f2cbdd49b62de10f969bbf9b","permalink":"/publication/16-text-recog-isvc/","publishdate":"2016-12-10T00:00:00Z","relpermalink":"/publication/16-text-recog-isvc/","section":"publication","summary":"Abstract: Scene text in indoor environments usually preserves and communicates important contextual information which can significantly enhance the independent travel of blind and visually impaired people. In this paper, we present an assistive text spotting navigation system based on an RGB-D mobile device for blind or severely visually impaired people.","tags":["Visual Computing","Sensing","Deep Learning","Human-Centered Computing"],"title":"Guided Text Spotting for Assistive Blind Navigation in Unfamiliar Indoor Environments","type":"publication"},{"authors":["Juan Pablo Munoz","Bing Li","Xuejian Rong","Jizhong Xiao","Yingli Tian","Aries Arditi"],"categories":null,"content":"Abstract: Research in Artificial Intelligence, Robotics and Computer Vision has recently made great strides in improving indoor localization. Publicly available technology now allows for indoor localization with very small margins of error. In this demo, we show a system that uses state-of the-art technology to assist visually impaired people navigate indoors. Our system takes advantage of spatial representations from CAD files, or floor plan images, to extract valuable information that later can be used to improve navigation and human-computer interaction. Using depth information, our system is capable of detecting obstacles and guiding the user to avoid them.\n","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"bdad7b75d6778dd37656af7ff85a4d59","permalink":"/publication/16-demo-navi-ijcai/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/publication/16-demo-navi-ijcai/","section":"publication","summary":"Abstract: Research in Artificial Intelligence, Robotics and Computer Vision has recently made great strides in improving indoor localization. Publicly available technology now allows for indoor localization with very small margins of error.","tags":null,"title":"Demo: Assisting Visually Impaired People Navigate Indoors","type":"publication"},{"authors":["Xiaochen Zhang","Jizhong Xiao","Bing Li","Juan Pablo Munoz","Samleo L. Joseph","Yi Sun","Chucai Yi","Yingli Tian"],"categories":null,"content":"Abstract: This paper presents a wearable indoor navigation system that helps visually impaired user to perform indoor navigation. The system takes advantage of the Simultaneous Localization and Mapping (SLAM) and semantic path planning to accomplish localization and navigation tasks while collaborating with the visually impaired user. It integrates multiple sensors and feedback devices as an RGB-D camera, an IMU and a web camera; and it applies the RGB-D based visual odometry algorithm to estimate the user\u0026rsquo;s location and orientation, and uses the IMU to refine the orientation error. Major landmarks such as room numbers and corridor corners are detected by the web camera and RGB-D camera, and matched to the digitalized floor map so as to localize the user. The path and motion guidance are generated afterwards to guide the user to a desired destination. To improve the fitting between the rigid commands and optimal machine decisions for human beings, we propose a context based decision making mechanism on path planning that resolves users\u0026rsquo; confusions caused by incorrect observations. The software modules are implemented in Robotics Operating System (ROS) and the navigation system are tested with blindfolded sight persons. The field experiments confirm the feasibility of the system prototype and the proposed mechanism.\n","date":1479168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479168000,"objectID":"58639d1a9d84f25bd626f70b0de24549","permalink":"/publication/16-wearable-navi-sys/","publishdate":"2016-11-15T00:00:00Z","relpermalink":"/publication/16-wearable-navi-sys/","section":"publication","summary":"Abstract: This paper presents a wearable indoor navigation system that helps visually impaired user to perform indoor navigation. The system takes advantage of the Simultaneous Localization and Mapping (SLAM) and semantic path planning to accomplish localization and navigation tasks while collaborating with the visually impaired user.","tags":["Robotics","Sensing","Human-Centered Computing"],"title":"A Wearable Indoor Navigation System with Context-based Decision Making For Visually Impaired","type":"publication"},{"authors":["Bing Li","Juan Pablo Munoz","Xuejian Rong","Jizhong Xiao","Yingli Tian","Aries Arditi"],"categories":null,"content":"Abstract: This paper presents a novel mobile wearable context-aware indoor maps and navigation system with obstacle avoidance for the blind. The system includes an indoor map editor and an App on Tango devices with multiple modules. The indoor map editor parses spatial semantic information from a building architectural model, and represents it as a high-level semantic map to support context awareness. An obstacle avoidance module detects objects in front using a depth sensor. Based on the ego-motion tracking within the Tango, localization alignment on the semantic map, and obstacle detection, the system automatically generates a safe path to a desired destination. A speech-audio interface delivers user input, guidance and alert cues in real-time using a priority-based mechanism to reduce the user’s cognitive load. Field tests involving blindfolded and blind subjects demonstrate that the proposed prototype performs context-aware and safety indoor assistive navigation effectively.\n","date":1478131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1478131200,"objectID":"72349b017dd549ff68ab6c4735794d06","permalink":"/publication/16-isana-navi-eccvw/","publishdate":"2016-11-03T00:00:00Z","relpermalink":"/publication/16-isana-navi-eccvw/","section":"publication","summary":"Abstract: This paper presents a novel mobile wearable context-aware indoor maps and navigation system with obstacle avoidance for the blind. The system includes an indoor map editor and an App on Tango devices with multiple modules.","tags":["Visual Computing","Human-Centered Computing"],"title":"ISANA: Wearable Context-Aware Indoor Assistive Navigation with Obstacle Avoidance for the Blind","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"fd54fddb66e6757b450fa0f734babdce","permalink":"/project/example-copy-2/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example-copy-2/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"cb03208fe4f6c09b81b856f38029d279","permalink":"/project/example-copy/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example-copy/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":["Xiaochen Zhang","Bing Li","Samleo L. Joseph","Jizhong Xiao","Yi Sun","Yingli Tian","Juan Pablo Munoz","Chucai Yi"],"categories":null,"content":"Abstract: This paper proposes a novel assistive navigation system based on simultaneous localization and mapping (SLAM) and semantic path planning to help visually impaired users navigate in indoor environments. The system integrates multiple wearable sensors and feedback devices including a RGB-D sensor and an inertial measurement unit (IMU) on the waist, a head mounted camera, a microphone and an earplug/speaker. We develop a visual odometry algorithm based on RGB-D data to estimate the user\u0026rsquo;s position and orientation, and refine the orientation error using the IMU. We employ the head mounted camera to recognize the door numbers and the RGB-D sensor to detect major landmarks such as corridor corners. By matching the detected landmarks against the corresponding features on the digitalized floor map, the system localizes the user, and provides verbal instruction to guide the user to the desired destination. The software modules of our system are implemented in Robotics Operating System (ROS). The prototype of the proposed assistive navigation system is evaluated by blindfolded sight persons. The field tests confirm the feasibility of the proposed algorithms and the system prototype.\n","date":1444348800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1444348800,"objectID":"6ff9ee1de180198072ddd4b0c63efbd8","permalink":"/publication/15-assistive-navi-smc/","publishdate":"2015-10-09T00:00:00Z","relpermalink":"/publication/15-assistive-navi-smc/","section":"publication","summary":"Abstract: This paper proposes a novel assistive navigation system based on simultaneous localization and mapping (SLAM) and semantic path planning to help visually impaired users navigate in indoor environments. The system integrates multiple wearable sensors and feedback devices including a RGB-D sensor and an inertial measurement unit (IMU) on the waist, a head mounted camera, a microphone and an earplug/speaker.","tags":null,"title":"A SLAM based Semantic Indoor Navigation System for Visually Impaired Users","type":"publication"},{"authors":["Jizhong Xiao","Samleo L. Joseph","Xiaochen Zhang","Bing Li","Xiaohai Li","Jianwei Zhang"],"categories":null,"content":"Abstract: This paper provides a framework for context-aware navigation services for vision impaired people. Integrating advanced intelligence into navigation requires knowledge of the semantic properties of the objects around the user\u0026rsquo;s environment. This interaction is required to enhance communication about objects and places to improve travel decisions. Our intelligent system is a human-in-the-loop cyber-physical system that interprets ubiquitous semantic entities by interacting with the physical world and the cyber domain, viz., 1) visual cues and distance sensing of material objects as line-of-sight interaction to interpret location-context information, and 2) data (tweets) from social media as event-based interaction to interpret situational vibes. The case study elaborates our proposed localization methods (viz., topological, landmark, metric, crowdsourced, and sound localization) for applications in way finding, way confirmation, user tracking, socialization, and situation alerts. Our pilot evaluation provides a proof of concept for an assistive navigation system.\n","date":1421193600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1421193600,"objectID":"13d48f5c1a0f3f0aeb887ff41f68377e","permalink":"/publication/15-assistive-navi-thms/","publishdate":"2015-01-14T00:00:00Z","relpermalink":"/publication/15-assistive-navi-thms/","section":"publication","summary":"Abstract: This paper provides a framework for context-aware navigation services for vision impaired people. Integrating advanced intelligence into navigation requires knowledge of the semantic properties of the objects around the user\u0026rsquo;s environment.","tags":null,"title":"An Assistive Navigation Framework for the Visually Impaired","type":"publication"},{"authors":["Bing Li","Jing Cao","Jizhong Xiao","Xiaochen Zhang","Hongfan Wang"],"categories":null,"content":"Abstract: Previous research has shown that the impact-echo emission signals contain information about the flaws of structural integrity and deterioration levels of concrete bridges. This paper presents a method of using the mobile robot equipped with an impact-echo Non-Destructive Evaluation (NDE) device to autonomously collect data, perform automatic classification and 3D visualization of the detected flaws. This method is based on Power Spectral Density (PSD) analysis for the Fast Fourier Transform (FFT) of the impact-echo signals, and Support Vector Machine (SVM) classification. Therefore, health condition of concrete bridge decks can be automatically recorded, analyzed and visualized in 3D.\n","date":1404000000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404000000,"objectID":"e45e6b9d9d40441e9dfac5f0edffa7dc","permalink":"/publication/14-impact-echo-wcica/","publishdate":"2014-06-29T00:00:00Z","relpermalink":"/publication/14-impact-echo-wcica/","section":"publication","summary":"Abstract: Previous research has shown that the impact-echo emission signals contain information about the flaws of structural integrity and deterioration levels of concrete bridges. This paper presents a method of using the mobile robot equipped with an impact-echo Non-Destructive Evaluation (NDE) device to autonomously collect data, perform automatic classification and 3D visualization of the detected flaws.","tags":null,"title":"Robotic Impact-Echo Non-Destructive Evaluation based on FFT and SVM","type":"publication"},{"authors":["Zhanggang Lu","Bing Li","Jianhua Wang","Weihai Chen"],"categories":null,"content":"Abstract: An improved virtual flexible bar algorithm which aims at improving the trajectory tracking accuracy of automatic vehicle following system is proposed in this paper. To achieve successful vehicle following, the algorithm mainly consists two parts: the flexible bar which can fit the leader vehicle\u0026rsquo;s trajectory and the force acting in the flexible bar which pull the follower vehicle to track the leader\u0026rsquo;s trajectory besides adjusting the velocity and acceleration of the follower according to the motion state of the leader. In order to maintain the smooth motion of the follower and make sure that the follower could carry out the commands transmitted from the controller accurately, an adaptive trajectory tracking control is also implemented. The simulated experimental results validate that the follower is able to trail the trajectories of the leader vehicle keep the error within an satisfied limits meanwhile maintaining a safe following distance.\n","date":1357603200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1357603200,"objectID":"1cfdbe7be42ffbf23b40dcb3176d8496","permalink":"/publication/13-vehicle-following/","publishdate":"2013-01-08T00:00:00Z","relpermalink":"/publication/13-vehicle-following/","section":"publication","summary":"Abstract: An improved virtual flexible bar algorithm which aims at improving the trajectory tracking accuracy of automatic vehicle following system is proposed in this paper. To achieve successful vehicle following, the algorithm mainly consists two parts: the flexible bar which can fit the leader vehicle\u0026rsquo;s trajectory and the force acting in the flexible bar which pull the follower vehicle to track the leader\u0026rsquo;s trajectory besides adjusting the velocity and acceleration of the follower according to the motion state of the leader.","tags":null,"title":"An Autonomous Vehicle Following Approach-The Virtual Flexible Bar Model","type":"publication"},{"authors":["Xianqiang You","Bing Li","Weihai Chen","Shouqian Yu"],"categories":null,"content":"Abstract: From shoulder to wrist, a person\u0026rsquo;s arm has a total of seven degrees of freedom (DOFs), which is the minimum number of DOF for the robot that needs to avoid obstacles and internal singularity. Compared to the traditional 7-DOF robot, the 7-DOF cable-driven robotic arm (CDRA) similar to human muscles parallel drive mode is the hybrid structure of serial-parallel, which possesses a number of promising advantages, such as simple and light-weight mechanical structure, high-loading capacity, and large reachable workspace. Since the cable can only generate tension and cannot stand pressure, cable-driven mechanism must be a redundant drive structure. Both the shoulder and wrist joint are redundant drive mechanism, and the tension of four cables has multiple solutions in the same posture. Hence, iteration Newton-Euler method is adopted to conduct dynamic analysis. Cable tension distribution algorithm based on null space method by solving Pseudo-inverse matrix is proposed, and in order to keep the cable tensional and also reduce energy consumption, the index of dynamic minimum pre-tightening force is originally proposed to realtime adjust the driving force of each cable. To show the accuracy and effectiveness of the proposed cable tension distribution algorithm, several simulation results are illustrated. These lay a good theory foundation for further research on effective control and performance improvement of the CDRA.\n","date":1323216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1323216000,"objectID":"260bedadf1983a9a5bf5f45a295e86f9","permalink":"/publication/11-robotic-arm-robio/","publishdate":"2011-12-07T00:00:00Z","relpermalink":"/publication/11-robotic-arm-robio/","section":"publication","summary":"Abstract: From shoulder to wrist, a person\u0026rsquo;s arm has a total of seven degrees of freedom (DOFs), which is the minimum number of DOF for the robot that needs to avoid obstacles and internal singularity.","tags":["Robotics"],"title":"Tension Distribution Algorithm of a 7-DOF Cable-Driven Robotic Arm Based on Dynamic Minimum Pre-Tightening Force","type":"publication"},{"authors":["Mingchen Gao","Weihai Chen","Bing Li","Tao Lv"],"categories":null,"content":"Abstract: This paper presents a 3D perceptual system based on 2D laser radar LMS291. The proposed system utilizes intelligent module to add a degree of freedom to LMS291. A new method of synchronization between LMS291 and intelligent module was proposed. We added Visual C++ multi-media timer to the timestamp method, the purpose we use this method is to change the level resolution conveniently, and control the number of the points we acquired, because in some condition we need speedy scanning to rebuild the reconstructed image of the terrain quickly. And we use Matlab to reconstruct the 2D and 3D images. To repair the singular points we selected absolute mean value method. The experiment shows that absolute mean value method works more accurately and effectively And this system we designed can be used for image reconstruction in unstructured environment by experiments.\n","date":1212624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1212624000,"objectID":"4ce608f9e933da059ecff74218621ff0","permalink":"/publication/08-perceptual-sys-iciea/","publishdate":"2008-06-05T00:00:00Z","relpermalink":"/publication/08-perceptual-sys-iciea/","section":"publication","summary":"Abstract: This paper presents a 3D perceptual system based on 2D laser radar LMS291. The proposed system utilizes intelligent module to add a degree of freedom to LMS291. A new method of synchronization between LMS291 and intelligent module was proposed.","tags":["Robotics","Visual Computing"],"title":"Construction and Realization of a 3D Perceptual System Based on 2D Laser Radar","type":"publication"},{"authors":["Jianhua Wang","Bing Li","Weihai Chen","Lixia Rong"],"categories":null,"content":"Abstract: For 3D reconstruction technology based on laser ranging for mobile robot, this paper presents the design of real-time embedded system for 3D data processing. Firstly, this paper introduces the home and abroad research situation of laser ranging for mobile robot, and analysis the method of 3D data acquisition with 2D laser scanner and ID driving motor module. Secondly, this paper introduces the hardware design and system building of multi-module embedded system, which consists of DSP data acquisition module, FPGA data processing module and ARM control. The ARM system is the main control system and realizes 3D reconstruction function. CAN bus communication is selected between these modules. Thirdly, this paper introduces the design of communication programming under ARM control system. At last, this paper presents the 3D reconstruction method with laser scanner and driving motor by Mesa3D based on RTLinux system.\n","date":1212451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1212451200,"objectID":"9fb56cad6270c968e86f70bb288359fe","permalink":"/publication/08-3d-recon-iciea/","publishdate":"2008-06-03T00:00:00Z","relpermalink":"/publication/08-3d-recon-iciea/","section":"publication","summary":"Abstract: For 3D reconstruction technology based on laser ranging for mobile robot, this paper presents the design of real-time embedded system for 3D data processing. Firstly, this paper introduces the home and abroad research situation of laser ranging for mobile robot, and analysis the method of 3D data acquisition with 2D laser scanner and ID driving motor module.","tags":["Robotics","Visual Computing"],"title":"3D Reconstruction Embedded System Based on Laser Scanner for Mobile Robot","type":"publication"},{"authors":null,"categories":["opening"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"905d9a79354e6aa5bea5e06c4398718c","permalink":"/post/_opening2-phd/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/_opening2-phd/","section":"post","summary":"","tags":null,"title":"","type":"post"},{"authors":null,"categories":["alumni"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1588e1334a97812e349a1da64f2ff589","permalink":"/post/aaron-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/aaron-chen/","section":"post","summary":"2022 Intern, currently a Master’s student in Automotive Engineering at CU-ICAR. Email: achen3 AT g.clemson.edu","tags":null,"title":"Aaron Chen","type":"post"},{"authors":null,"categories":["Ph.D. Students"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a7dbceadb8573208b5afe515001a8d09","permalink":"/post/abhishek-sharma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/abhishek-sharma/","section":"post","summary":"(2022~) Research focus: Embodied AI, Autonomous Mapping-Navigation, etc. Email: asharm5 AT clemson.edu ([Profile Page](https://www.linkedin.com/in/abhishek-shar-ma/))","tags":null,"title":"Abhishek Sharma","type":"post"},{"authors":null,"categories":["alumni"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e89cb457f907d4c803e7249c48a2c16c","permalink":"/post/akshay-bhagat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/akshay-bhagat/","section":"post","summary":"2018~2019 MS Graduate, then Mitsubishi Electric Automotive America, Inc. - Planning and Controls Engineer (Autonomous Driving/ADAS)","tags":null,"title":"Akshay Bhagat","type":"post"},{"authors":null,"categories":["Alumni"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"315ef9bc88aba3b90588d4cec7e9179a","permalink":"/post/akshay-desai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/akshay-desai/","section":"post","summary":"2018~2019 MS Graduate, then Magna International - Algorithm Engineer – Planning and Controls.","tags":null,"title":"Akshay Desai","type":"post"},{"authors":null,"categories":["alumni"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0387b427db2d8c2be06217011f876531","permalink":"/post/amogh-v.-reddy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/amogh-v.-reddy/","section":"post","summary":"2022~2023 MS Graduate, then Apex Systems - Robotics Engineer. MS Thesis: Virtual Prototyping and Validation for Autonomous Assistive Mobility. Email: amoghv AT g.clemson.edu","tags":null,"title":"Amogh Venkataramana Reddy","type":"post"},{"authors":null,"categories":["alumni"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"28f1c1aa65a19b3c0cfd05506869e0d8","permalink":"/post/aryan-naveen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/aryan-naveen/","section":"post","summary":"2019~2022 Intern, then Undergraduate Computer Science Program at Harvard University.","tags":null,"title":"Aryan Naveen","type":"post"},{"authors":null,"categories":["Ph.D. Students"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f56ee2181a123d7eb722db3780478603","permalink":"/post/cigdem-kokenoz/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/cigdem-kokenoz/","section":"post","summary":"(2022~) Research focus: Spatial-AI Simultaneous Localization and Mapping (SLAM) for Autonomous Robots/Vehicles, etc. Email: ckokeno AT clemson.edu ([Profile Page](https://www.linkedin.com/in/cigdem-kokenoz-840897105/))","tags":null,"title":"Cigdem Kokenoz","type":"post"},{"authors":null,"categories":["People"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ae7bb4fae9b53cef715bd48d22ec4d14","permalink":"/post/bing-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/bing-li/","section":"post","summary":"is an Assistant Professor of [Automotive Engineering](https://www.clemson.edu/cecas/departments/automotive-engineering) at [Clemson University](https://www.clemson.edu/) [International Center for Automotive Research (CU-ICAR)](https://cuicar.com/) since Aug. 2018. He founded and directs the [AutoAI Lab](https://autoai.us) research group. He received his Ph.D. degree in Electrical Engineering in 2018 from The City College (CCNY), at The City University of New York (CUNY). Email: bli4 AT clemson.edu, ([Profile Page](http://www.clemson.edu/cecas/departments/automotive-engineering/people/li.html) and [ Google Scholar](https://scholar.google.com/citations?user=yysOczkAAAAJ\u0026hl=en).)","tags":null,"title":"Dr. Bing Li","type":"post"},{"authors":null,"categories":["alumni"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5f69cc18807e73bad8c65c884cf7615b","permalink":"/post/howard-brand/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/howard-brand/","section":"post","summary":"2017~2022: Ph.D. graduate, then Machine Learning - Analytics Associate Manager at Accenture Federal Services. Research areas: Robotic Inspection, Nondestructive Evaluation (NDE), Remote/UAV Sensing, Radar Imaging and Signal Processing. Dissertation: Non-Destructive Terrain Evaluation and Modeling for Off-Road Autonomy. Email: hbrand AT g.clemson.edu","tags":null,"title":"Dr. Howard Brand","type":"post"},{"authors":null,"categories":["alumni"],"content":"Dr. Jesse F. Leaman is a Postdoctoral Research Fellow in the AutoAI Lab at the Department of Automotive Engineering, Clemson University International Center for Automotive Research (CU-ICAR) since Feb. 2022. He received his Ph.D. in Astrophysics in 2008 at the University of California, Berkeley. His research interests include AI for Astrophysics, Robotic Astronomy, Robotic Wheelchairs and Assistive Mobility, Human-Robot Interaction, Human-centered AI, AI for Climate. Contact Email: jfleama@clemson.edu\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"11e9f0bc2c5c32305a71549e0691e2e1","permalink":"/post/jesse-leaman/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/jesse-leaman/","section":"post","summary":"2022: was a Postdoctoral Research Fellow in the AutoAI Lab at the Department of Automotive Engineering, Clemson University International Center for Automotive Research (CU-ICAR). He received his Ph.D. in Astrophysics in 2008 at the University of California, Berkeley. His research interests include AI for Astrophysics, Robotic Astronomy, Robotic Wheelchairs and Assistive Mobility, Human-Robot Interaction, Human-centered AI, AI for Climate. Email: jfl0132 AT gmail.com","tags":null,"title":"Dr. Jesse F. Leaman","type":"post"},{"authors":null,"categories":["Alumni"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fafeb65ae454dad2f6340120f103108a","permalink":"/post/mingzhe-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/mingzhe-liu/","section":"post","summary":"2021 MS Graduate, then Ph.D. student at Arizona State University.","tags":null,"title":"Mingzhe Liu","type":"post"},{"authors":null,"categories":["Alumni"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d17322191721f40c22b146c33faedd23","permalink":"/post/minni-bin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/minni-bin/","section":"post","summary":"2020~2021 MS Graduate, then Toyota North America R\u0026D - ADAS Evaluation Engineer.","tags":null,"title":"Minni Bin","type":"post"},{"authors":null,"categories":["Alumni"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a7566c961570766540b362ed589ef2d4","permalink":"/post/nachiappan-chockalingam/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/nachiappan-chockalingam/","section":"post","summary":"2019 Intern, then Amazon AWS - Software Engineer.","tags":null,"title":"Nachiappan Chockalingam","type":"post"},{"authors":null,"categories":["Research Interns"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"12c571fa742da66d0b8ec414bd3346fc","permalink":"/post/nia-alavandi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/nia-alavandi/","section":"post","summary":"is currently a high school student at JL Mann High School, Greenville, SC. Email: nia.alava AT gmail.com","tags":null,"title":"Nia Alavandi","type":"post"},{"authors":null,"categories":["Ph.D. Students"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b4cad7f4985214e9d2f363c7a38bc6dc","permalink":"/post/nobel-dang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/nobel-dang/","section":"post","summary":"(2023~) Research focus: Autonomous AI. Email: ndang AT clemson.edu ([Profile Page](https://www.linkedin.com/in/nobeldang/))","tags":null,"title":"Nobel Dang","type":"post"},{"authors":null,"categories":["Alumni"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"409b056d0fc6e1be5e6350f4e4ff5aae","permalink":"/post/rahil-modi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/rahil-modi/","section":"post","summary":"2020~2021 MS Graduate, then Volvo - Vehicle System Engineer in the Automated Driving.","tags":null,"title":"Rahil Modi","type":"post"},{"authors":null,"categories":["Alumni"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0a08b9bb6010a6a2557e1f95238a5d5e","permalink":"/post/sanket-bachuwa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/sanket-bachuwa/","section":"post","summary":"2018~2019 MS Graduate, then ThorDrive - Autonomous Driving Engineer.","tags":null,"title":"Sanket Bachuwa","type":"post"},{"authors":null,"categories":["Alumni"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3138fe32da361453688c4c6390a7b369","permalink":"/post/shaurya-panthri/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/shaurya-panthri/","section":"post","summary":"2019~2020 MS Graduate, then Toyota North America R\u0026D - Automated Driving Software Engineer.","tags":null,"title":"Shaurya Panthri","type":"post"},{"authors":null,"categories":["Alumni"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9e401b2f1e5862eb706dd82c8eb485a6","permalink":"/post/srivatsav-kamarajugadda/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/srivatsav-kamarajugadda/","section":"post","summary":"2018~2019 MS Graduate, then NVIDIA - Senior Planning and Controls Engineer.","tags":null,"title":"Srivatsav Kamarajugadda","type":"post"},{"authors":null,"categories":["Alumni"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ad8555c743278a5102c89f89ba2d9b71","permalink":"/post/tarun-giridhar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/tarun-giridhar/","section":"post","summary":"2020~2021 Intern, then Undergraduate Computer Science Program at Virginia Tech.","tags":null,"title":"Tarun Giridhar","type":"post"},{"authors":null,"categories":["opening"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"af2a93aa7934fe450f0456cc99a1f886","permalink":"/post/_opening3-msthesis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/_opening3-msthesis/","section":"post","summary":"- Undergraduate or Master thesis projects;\n- related to robotics/vehicle sensing, perception, machine/deep learning and autonomy;","tags":null,"title":"Thesis Advising Projects for BS/MS Students","type":"post"},{"authors":null,"categories":["Research Interns"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0744fa9d8d8f03cddc89ca8197b44021","permalink":"/post/toukheer-shaik/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/toukheer-shaik/","section":"post","summary":"is currently a Master’s student in Automotive Engineering at CU-ICAR. Email: tshaik AT g.clemson.edu","tags":null,"title":"Toukheer Shaik","type":"post"},{"authors":null,"categories":["opening"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ff0baca07836f5dc27eb7db3d20d6862","permalink":"/post/_opening4-interns/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/_opening4-interns/","section":"post","summary":"\n- K-12, undergraduate and graduate student interns;\n- Visiting Ph.D. students;\n- Visiting scholars;","tags":null,"title":"Visiting and Internship Opportunities","type":"post"},{"authors":null,"categories":["Alumni"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fd3d4f6666cb3904329638f1572040ab","permalink":"/post/zack-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/zack-yang/","section":"post","summary":"2020~2022 MS Graduate, then Lucid Motors - Software Engineer (Autonomous Driving/ADAS)","tags":null,"title":"Zack Yang","type":"post"},{"authors":null,"categories":["Ph.D. Students"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d91a9cb15ef922e1ff6695f6842cf966","permalink":"/post/zhimin-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/zhimin-chen/","section":"post","summary":"(2020~) Research focus: Perception AI for Autonomous Robots/Vehicles, etc. Email: zhiminc AT clemson.edu ([Profile Page](https://zhiminc.website/))","tags":null,"title":"Zhimin Chen","type":"post"},{"authors":null,"categories":["Ph.D. Students"],"content":"Email: zfeng@clemson.edu Homepage: ziyue.cool\nBiography Ziyue Feng is a Ph.D. student at Clemson University. His research interests include, 3D Computer vision, Deep Learning,and Autonomous. He is now focusing on handeling dynamic objects in 3D perception.\nInterests  Depth Estimation 3D Computer vision Deep Learning Autonomous  Publications:   Disentangling Object Motion and Occlusion for Unsupervised Multi-frame Monocular Depth  Advancing Self-Supervised Monocular Depth Learning with Sparse LiDAR  PSE-Match: A Viewpoint-Free Place Recognition Method With Parallel Semantic Embedding  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8c25a57ca6b44fa5d4986880080e74d2","permalink":"/post/ziyue-feng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/ziyue-feng/","section":"post","summary":"(2019~) Research focus: Neural Mapping for Autonomous Robots/Vehicles, etc. Email: zfeng AT clemson.edu (Profile Page: [ziyue.cool](https://ziyue.cool))","tags":null,"title":"Ziyue Feng","type":"post"}]